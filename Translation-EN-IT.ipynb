{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6YzZrevmfoe",
        "outputId": "27ab9e9d-6b3e-4073-dc9f-954da750c47b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3TT-iJemn8L",
        "outputId": "941350cf-694f-42b3-9886-32902faddd9e"
      },
      "source": [
        "%cd \"drive/MyDrive/Transformers/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MlINra1lRz2"
      },
      "source": [
        "# !wget \"https://www.statmt.org/europarl/v7/it-en.tgz\"\n",
        "# !tar -xvzf it-en.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxGj6hVVlR15"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1rxO7zPnd5P"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv5Y3kEDsmFE"
      },
      "source": [
        "with open(\"europarl-v7.it-en.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as my_file:\n",
        "          europal_en = my_file.read()\n",
        "\n",
        "with open(\"europarl-v7.it-en.it\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as my_file:\n",
        "          europal_it = my_file.read()\n",
        "\n",
        "with open(\"nonbreaking_prefix.en.txt\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as my_file:\n",
        "          non_breaking_prefix_en = my_file.read()\n",
        "\n",
        "with open(\"nonbreaking_prefix.it.txt\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as my_file:\n",
        "          non_breaking_prefix_it = my_file.read()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHI7JJkftf1h"
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [\" \" + pref + \".\" for pref in non_breaking_prefix_en if pref != \"\"]\n",
        "non_breaking_prefix_it = non_breaking_prefix_it.split(\"\\n\")\n",
        "non_breaking_prefix_it = [\" \" + pref + \".\" for pref in non_breaking_prefix_it if pref != \"\"]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN-hPhQQtirm"
      },
      "source": [
        "corpus_en = europal_en\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix,prefix + \"###\")\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\",\".###\",corpus_en)\n",
        "corpus_en = re.sub(r'\\.###',\"\",corpus_en)\n",
        "corpus_en = re.sub(r\"  +\",\" \",corpus_en)\n",
        "corpus_en = corpus_en.split(\"\\n\")\n",
        "\n",
        "corpus_it = europal_it\n",
        "for prefix in non_breaking_prefix_it:\n",
        "    corpus_it = corpus_it.replace(prefix,prefix + \"###\")\n",
        "corpus_it = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\",\".###\",corpus_it)\n",
        "corpus_it = re.sub(r'\\.###',\"\",corpus_it)\n",
        "corpus_it = re.sub(r\"  +\",\" \",corpus_it)\n",
        "corpus_it = corpus_it.split(\"\\n\")   "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDcLPXiQNjFV"
      },
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_it = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_it, target_vocab_size=2**13)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "795WeS5nUOaA"
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_IT = tokenizer_it.vocab_size + 2"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sGYQ2NeUk4E"
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1] \n",
        "          for sentence in corpus_en]\n",
        "\n",
        "outputs = [[VOCAB_SIZE_IT-2] + tokenizer_it.encode(sentence) + [VOCAB_SIZE_IT-1] \n",
        "          for sentence in corpus_it]          "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNfMlqzUk6Z"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count,sentence in enumerate(inputs)\n",
        "                if len(sentence) > MAX_LENGTH] \n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "idx_to_remove = [count for count,sentence in enumerate(outputs)\n",
        "                if len(sentence) > MAX_LENGTH] \n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4U4rXy6Uk8t"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8z0xsf8Uk_g"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "datasets = tf.data.Dataset.from_tensor_slices((inputs,outputs))\n",
        "\n",
        "datasets = datasets.cache()\n",
        "datasets = datasets.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "datasets = datasets.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFjJH5JjgCN6"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding,self).__init__()\n",
        "\n",
        "    def get_angles(self,pos,i,d_model): # pos:(seq_length,1) i:(1,d_model) \n",
        "        angles = 1 / np.power(10000.0,(2*(i//2))/np.float32(d_model))\n",
        "        return pos * angles # (seq_length,d_model)\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:,np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis,:],\n",
        "                                 d_model)\n",
        "        angles[:,0::2] = np.sin(angles[:,0::2])\n",
        "        angles[:,1::2] = np.cos(angles[:,1::2])\n",
        "        pos_encoding = angles[np.newaxis,...]\n",
        "        return inputs + tf.cast(pos_encoding,tf.float32)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfV2Bu67n_GU"
      },
      "source": [
        "def scaled_dor_product_attention(queris,keys,values,mask):\n",
        "    product = tf.matmul(queris,keys,transpose_b=True)\n",
        "\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1],tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "\n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product,axis=-1),values)\n",
        "    return attention"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef5TCUBgn_Lt"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self,nb_proj):\n",
        "        super(MultiHeadAttention,self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert (self.d_model % self.nb_proj) == 0\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "        self.query_lin = layers.Dense(self.d_model)\n",
        "        self.keys_lin = layers.Dense(self.d_model)\n",
        "        self.values_lin = layers.Dense(self.d_model)\n",
        "\n",
        "        self.final_lin = layers.Dense(self.d_model)\n",
        "\n",
        "    def split_proj(self,inputs,batch_size): # inputs: (batch_size,seq_length,d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "\n",
        "        splited_inputs = tf.reshape(inputs,shape=shape) # (batch_size,seq_length,nb_proj,d_proj)\n",
        "        return tf.transpose(splited_inputs,[0,2,1,3]) # (batch_size,nb_proj,seq_length,d_proj)\n",
        "\n",
        "    def call(self,queries,keys,values,mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.keys_lin(keys)\n",
        "        values = self.values_lin(values)\n",
        "\n",
        "        queries = self.split_proj(queries,batch_size)\n",
        "        keys = self.split_proj(keys,batch_size)\n",
        "        values = self.split_proj(values,batch_size)\n",
        "\n",
        "        attention = scaled_dor_product_attention(queries,keys,values,mask)\n",
        "        attention = tf.transpose(attention,[0,2,1,3])\n",
        "        shape = (batch_size,-1,self.d_model)\n",
        "        concat_attention = tf.reshape(attention,shape=shape)\n",
        "\n",
        "        outputs = self.final_lin(concat_attention)\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ihQdcU7rzht"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self,FFN_units,nb_proj,dropout):\n",
        "        super(EncoderLayer,self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,activation='relu')\n",
        "        self.dense_2 = layers.Dense(units=self.d_model,activation='relu')\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,inputs,inputs,mask)\n",
        "        attention = self.dropout_1(attention,training=training)\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs,training=training)\n",
        "        outputs = self.norm_2(outputs+attention)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpX2Efdqr058"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder,self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size,d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [\n",
        "                           EncoderLayer(FFN_units,nb_proj,dropout) \n",
        "                           for _ in range(self.nb_layers)\n",
        "                        ]\n",
        "\n",
        "    def call(self,inputs,mask,training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs,training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs,mask,training)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft1O7w1f54er"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self,FFN_units,nb_proj,dropout):\n",
        "        super(DecoderLayer,self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "    \n",
        "    def build(self,input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,activation='relu')\n",
        "        self.dense_2 = layers.Dense(units=self.d_model,activation='relu')\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout)    \n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self,inputs,enc_outputs,mask_1,mask_2,training):\n",
        "        attention = self.multi_head_attention_1(inputs,inputs,inputs,mask_1)\n",
        "        attention = self.dropout_1(attention,training)\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2) \n",
        "        attention_2 = self.dropout_2(attention_2,training)\n",
        "        attention_2 = self.norm_2(attention_2+attention)\n",
        "\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs,training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY0eEbGl54b8"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name='decoder'):\n",
        "        super(Decoder,self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        self.embedding = layers.Embedding(vocab_size,d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "\n",
        "        self.dec_layers = [\n",
        "                           DecoderLayer(FFN_units,nb_proj,dropout)\n",
        "                           for _ in range(nb_layers)]\n",
        "        \n",
        "\n",
        "    def call(self,inputs,enc_outputs,mask_1,mask_2,training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *=  tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs,training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)      \n",
        "        return outputs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPZ75KXMH-Lb"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer,self).__init__(name=name)\n",
        "        self.encoder =  Encoder(nb_layers,\n",
        "                                FFN_units,\n",
        "                                nb_proj,\n",
        "                                dropout,\n",
        "                                vocab_size_enc,\n",
        "                                d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "    \n",
        "    def create_padding_mask(self,seq): # (batch_size,seq_length)\n",
        "        mask = tf.cast(tf.math.equal(seq,0),tf.float32)\n",
        "        return mask[:,tf.newaxis,tf.newaxis,:]\n",
        "\n",
        "    def create_look_ahead_mask(self,seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len,seq_len)),-1,0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self,enc_inputs,dec_inputs,training):\n",
        "        enc_mask = self.create_look_ahead_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        enc_outputs = self.encoder(enc_inputs,enc_mask,training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kihxe-dH-o9"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "D_MODEL = 128\n",
        "NB_LAYERS = 4\n",
        "FFN_UNITS = 512\n",
        "NB_PROJ = 8\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc = VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec = VOCAB_SIZE_IT,\n",
        "                          d_model = D_MODEL,\n",
        "                          nb_layers = NB_LAYERS,\n",
        "                          FFN_units = FFN_UNITS,\n",
        "                          nb_proj = NB_PROJ,\n",
        "                          dropout = DROPOUT_RATE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAVSKf5yRtjs"
      },
      "source": [
        "def custom_sparse_categorical_accuracy(y_true, y_pred):\n",
        "    return K.cast(K.equal(K.max(y_true, axis=-1),\n",
        "                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n",
        "                  K.floatx())\n",
        "    \n",
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
        "                                                          logits=y_pred)\n",
        "    return loss"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd0cMs7QH-rL"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "def loss_function(target,prediction):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target,0))\n",
        "\n",
        "    loss_ = loss_object(target,prediction)\n",
        "    \n",
        "    mask = tf.cast(mask,dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YreMF3rm0Ya9"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self,d_model,warmup_steps=4000):\n",
        "        super(CustomSchedule,self).__init__()\n",
        "        self.d_model = tf.cast(d_model,tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self,step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1,arg2)\n",
        "    \n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtjlAFu-0aB1"
      },
      "source": [
        "checkpoint_path = './Checkpoints'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint_path,max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint restored\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA9-j7bo8Qoe",
        "outputId": "899525da-432f-4450-958b-33ac6fc4074e"
      },
      "source": [
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Start of epoch {epoch+1}\")\n",
        "    start = time.time()\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    for (batch,(enc_inputs,targets)) in enumerate(datasets):\n",
        "        dec_inputs = targets[:,:-1]\n",
        "        dec_outputs_real = targets[:,1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs,dec_inputs,True)\n",
        "            loss = loss_function(dec_outputs_real,predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss,transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients,transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real,predictions)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4} Accuracy {:0.4f}\".format(\n",
        "                epoch+1,batch,train_loss.result(),train_accuracy.result()))\n",
        "    \n",
        "\n",
        "\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoints for epoch {} at {}\".format(epoch+1,ckpt_save_path))\n",
        "    end = time.time()\n",
        "    print(\"Time taken for 1 epoch: {} secs\".format(end-start))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 5.845 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.109 Accuracy 0.0119\n",
            "Epoch 1 Batch 100 Loss 6.041 Accuracy 0.0321\n",
            "Epoch 1 Batch 150 Loss 5.964 Accuracy 0.0389\n",
            "Epoch 1 Batch 200 Loss 5.893 Accuracy 0.0423\n",
            "Epoch 1 Batch 250 Loss 5.794 Accuracy 0.0444\n",
            "Epoch 1 Batch 300 Loss 5.689 Accuracy 0.0457\n",
            "Epoch 1 Batch 350 Loss 5.579 Accuracy 0.0469\n",
            "Epoch 1 Batch 400 Loss 5.468 Accuracy 0.0507\n",
            "Epoch 1 Batch 450 Loss 5.359 Accuracy 0.0552\n",
            "Epoch 1 Batch 500 Loss 5.26 Accuracy 0.0591\n",
            "Epoch 1 Batch 550 Loss 5.174 Accuracy 0.0627\n",
            "Epoch 1 Batch 600 Loss 5.093 Accuracy 0.0664\n",
            "Epoch 1 Batch 650 Loss 5.017 Accuracy 0.0702\n",
            "Epoch 1 Batch 700 Loss 4.947 Accuracy 0.0744\n",
            "Epoch 1 Batch 750 Loss 4.882 Accuracy 0.0785\n",
            "Epoch 1 Batch 800 Loss 4.817 Accuracy 0.0825\n",
            "Epoch 1 Batch 850 Loss 4.754 Accuracy 0.0865\n",
            "Epoch 1 Batch 900 Loss 4.696 Accuracy 0.0903\n",
            "Epoch 1 Batch 950 Loss 4.636 Accuracy 0.0941\n",
            "Epoch 1 Batch 1000 Loss 4.58 Accuracy 0.0977\n",
            "Epoch 1 Batch 1050 Loss 4.527 Accuracy 0.1012\n",
            "Epoch 1 Batch 1100 Loss 4.479 Accuracy 0.1045\n",
            "Epoch 1 Batch 1150 Loss 4.431 Accuracy 0.1077\n",
            "Epoch 1 Batch 1200 Loss 4.386 Accuracy 0.1107\n",
            "Epoch 1 Batch 1250 Loss 4.342 Accuracy 0.1138\n",
            "Epoch 1 Batch 1300 Loss 4.299 Accuracy 0.1168\n",
            "Epoch 1 Batch 1350 Loss 4.258 Accuracy 0.1199\n",
            "Epoch 1 Batch 1400 Loss 4.219 Accuracy 0.1229\n",
            "Epoch 1 Batch 1450 Loss 4.182 Accuracy 0.1259\n",
            "Epoch 1 Batch 1500 Loss 4.144 Accuracy 0.1288\n",
            "Epoch 1 Batch 1550 Loss 4.109 Accuracy 0.1316\n",
            "Epoch 1 Batch 1600 Loss 4.073 Accuracy 0.1345\n",
            "Epoch 1 Batch 1650 Loss 4.04 Accuracy 0.1372\n",
            "Epoch 1 Batch 1700 Loss 4.007 Accuracy 0.1398\n",
            "Epoch 1 Batch 1750 Loss 3.975 Accuracy 0.1424\n",
            "Epoch 1 Batch 1800 Loss 3.944 Accuracy 0.1450\n",
            "Epoch 1 Batch 1850 Loss 3.914 Accuracy 0.1476\n",
            "Epoch 1 Batch 1900 Loss 3.884 Accuracy 0.1501\n",
            "Epoch 1 Batch 1950 Loss 3.854 Accuracy 0.1524\n",
            "Epoch 1 Batch 2000 Loss 3.825 Accuracy 0.1547\n",
            "Epoch 1 Batch 2050 Loss 3.796 Accuracy 0.1570\n",
            "Epoch 1 Batch 2100 Loss 3.767 Accuracy 0.1593\n",
            "Epoch 1 Batch 2150 Loss 3.738 Accuracy 0.1616\n",
            "Epoch 1 Batch 2200 Loss 3.709 Accuracy 0.1638\n",
            "Epoch 1 Batch 2250 Loss 3.681 Accuracy 0.1662\n",
            "Epoch 1 Batch 2300 Loss 3.654 Accuracy 0.1684\n",
            "Epoch 1 Batch 2350 Loss 3.627 Accuracy 0.1706\n",
            "Epoch 1 Batch 2400 Loss 3.6 Accuracy 0.1728\n",
            "Epoch 1 Batch 2450 Loss 3.575 Accuracy 0.1751\n",
            "Epoch 1 Batch 2500 Loss 3.55 Accuracy 0.1774\n",
            "Epoch 1 Batch 2550 Loss 3.526 Accuracy 0.1796\n",
            "Epoch 1 Batch 2600 Loss 3.501 Accuracy 0.1818\n",
            "Epoch 1 Batch 2650 Loss 3.477 Accuracy 0.1841\n",
            "Epoch 1 Batch 2700 Loss 3.453 Accuracy 0.1864\n",
            "Epoch 1 Batch 2750 Loss 3.43 Accuracy 0.1888\n",
            "Epoch 1 Batch 2800 Loss 3.407 Accuracy 0.1909\n",
            "Epoch 1 Batch 2850 Loss 3.385 Accuracy 0.1931\n",
            "Epoch 1 Batch 2900 Loss 3.363 Accuracy 0.1951\n",
            "Epoch 1 Batch 2950 Loss 3.341 Accuracy 0.1971\n",
            "Epoch 1 Batch 3000 Loss 3.321 Accuracy 0.1992\n",
            "Epoch 1 Batch 3050 Loss 3.3 Accuracy 0.2011\n",
            "Epoch 1 Batch 3100 Loss 3.281 Accuracy 0.2030\n",
            "Epoch 1 Batch 3150 Loss 3.261 Accuracy 0.2048\n",
            "Epoch 1 Batch 3200 Loss 3.241 Accuracy 0.2067\n",
            "Epoch 1 Batch 3250 Loss 3.223 Accuracy 0.2085\n",
            "Epoch 1 Batch 3300 Loss 3.204 Accuracy 0.2102\n",
            "Epoch 1 Batch 3350 Loss 3.186 Accuracy 0.2119\n",
            "Epoch 1 Batch 3400 Loss 3.169 Accuracy 0.2136\n",
            "Epoch 1 Batch 3450 Loss 3.152 Accuracy 0.2153\n",
            "Epoch 1 Batch 3500 Loss 3.135 Accuracy 0.2170\n",
            "Epoch 1 Batch 3550 Loss 3.119 Accuracy 0.2186\n",
            "Epoch 1 Batch 3600 Loss 3.101 Accuracy 0.2203\n",
            "Epoch 1 Batch 3650 Loss 3.086 Accuracy 0.2219\n",
            "Epoch 1 Batch 3700 Loss 3.07 Accuracy 0.2236\n",
            "Epoch 1 Batch 3750 Loss 3.055 Accuracy 0.2251\n",
            "Epoch 1 Batch 3800 Loss 3.039 Accuracy 0.2266\n",
            "Epoch 1 Batch 3850 Loss 3.024 Accuracy 0.2282\n",
            "Epoch 1 Batch 3900 Loss 3.01 Accuracy 0.2297\n",
            "Epoch 1 Batch 3950 Loss 2.995 Accuracy 0.2312\n",
            "Epoch 1 Batch 4000 Loss 2.981 Accuracy 0.2326\n",
            "Epoch 1 Batch 4050 Loss 2.968 Accuracy 0.2339\n",
            "Epoch 1 Batch 4100 Loss 2.956 Accuracy 0.2351\n",
            "Epoch 1 Batch 4150 Loss 2.944 Accuracy 0.2363\n",
            "Epoch 1 Batch 4200 Loss 2.933 Accuracy 0.2374\n",
            "Epoch 1 Batch 4250 Loss 2.922 Accuracy 0.2385\n",
            "Epoch 1 Batch 4300 Loss 2.911 Accuracy 0.2396\n",
            "Epoch 1 Batch 4350 Loss 2.901 Accuracy 0.2406\n",
            "Epoch 1 Batch 4400 Loss 2.89 Accuracy 0.2416\n",
            "Epoch 1 Batch 4450 Loss 2.881 Accuracy 0.2425\n",
            "Epoch 1 Batch 4500 Loss 2.871 Accuracy 0.2435\n",
            "Epoch 1 Batch 4550 Loss 2.861 Accuracy 0.2444\n",
            "Epoch 1 Batch 4600 Loss 2.851 Accuracy 0.2454\n",
            "Epoch 1 Batch 4650 Loss 2.842 Accuracy 0.2463\n",
            "Epoch 1 Batch 4700 Loss 2.832 Accuracy 0.2472\n",
            "Epoch 1 Batch 4750 Loss 2.823 Accuracy 0.2481\n",
            "Epoch 1 Batch 4800 Loss 2.814 Accuracy 0.2490\n",
            "Epoch 1 Batch 4850 Loss 2.804 Accuracy 0.2499\n",
            "Epoch 1 Batch 4900 Loss 2.796 Accuracy 0.2508\n",
            "Epoch 1 Batch 4950 Loss 2.787 Accuracy 0.2517\n",
            "Epoch 1 Batch 5000 Loss 2.778 Accuracy 0.2525\n",
            "Epoch 1 Batch 5050 Loss 2.77 Accuracy 0.2533\n",
            "Epoch 1 Batch 5100 Loss 2.761 Accuracy 0.2541\n",
            "Epoch 1 Batch 5150 Loss 2.752 Accuracy 0.2549\n",
            "Epoch 1 Batch 5200 Loss 2.744 Accuracy 0.2557\n",
            "Epoch 1 Batch 5250 Loss 2.735 Accuracy 0.2564\n",
            "Epoch 1 Batch 5300 Loss 2.727 Accuracy 0.2571\n",
            "Epoch 1 Batch 5350 Loss 2.719 Accuracy 0.2579\n",
            "Epoch 1 Batch 5400 Loss 2.711 Accuracy 0.2586\n",
            "Epoch 1 Batch 5450 Loss 2.702 Accuracy 0.2594\n",
            "Epoch 1 Batch 5500 Loss 2.694 Accuracy 0.2601\n",
            "Epoch 1 Batch 5550 Loss 2.686 Accuracy 0.2608\n",
            "Saving checkpoints for epoch 1 at ./Checkpoints/ckpt-1\n",
            "Time taken for 1 epoch: 1487.2306196689606 secs\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 2.023 Accuracy 0.3512\n",
            "Epoch 2 Batch 50 Loss 1.851 Accuracy 0.3471\n",
            "Epoch 2 Batch 100 Loss 1.846 Accuracy 0.3455\n",
            "Epoch 2 Batch 150 Loss 1.836 Accuracy 0.3457\n",
            "Epoch 2 Batch 200 Loss 1.832 Accuracy 0.3467\n",
            "Epoch 2 Batch 250 Loss 1.822 Accuracy 0.3471\n",
            "Epoch 2 Batch 300 Loss 1.819 Accuracy 0.3468\n",
            "Epoch 2 Batch 350 Loss 1.816 Accuracy 0.3469\n",
            "Epoch 2 Batch 400 Loss 1.808 Accuracy 0.3477\n",
            "Epoch 2 Batch 450 Loss 1.806 Accuracy 0.3480\n",
            "Epoch 2 Batch 500 Loss 1.803 Accuracy 0.3484\n",
            "Epoch 2 Batch 550 Loss 1.801 Accuracy 0.3483\n",
            "Epoch 2 Batch 600 Loss 1.799 Accuracy 0.3489\n",
            "Epoch 2 Batch 650 Loss 1.793 Accuracy 0.3498\n",
            "Epoch 2 Batch 700 Loss 1.79 Accuracy 0.3505\n",
            "Epoch 2 Batch 750 Loss 1.785 Accuracy 0.3516\n",
            "Epoch 2 Batch 800 Loss 1.779 Accuracy 0.3524\n",
            "Epoch 2 Batch 850 Loss 1.773 Accuracy 0.3534\n",
            "Epoch 2 Batch 900 Loss 1.767 Accuracy 0.3544\n",
            "Epoch 2 Batch 950 Loss 1.762 Accuracy 0.3553\n",
            "Epoch 2 Batch 1000 Loss 1.757 Accuracy 0.3561\n",
            "Epoch 2 Batch 1050 Loss 1.75 Accuracy 0.3568\n",
            "Epoch 2 Batch 1100 Loss 1.743 Accuracy 0.3577\n",
            "Epoch 2 Batch 1150 Loss 1.738 Accuracy 0.3585\n",
            "Epoch 2 Batch 1200 Loss 1.734 Accuracy 0.3595\n",
            "Epoch 2 Batch 1250 Loss 1.73 Accuracy 0.3605\n",
            "Epoch 2 Batch 1300 Loss 1.724 Accuracy 0.3614\n",
            "Epoch 2 Batch 1350 Loss 1.721 Accuracy 0.3625\n",
            "Epoch 2 Batch 1400 Loss 1.715 Accuracy 0.3634\n",
            "Epoch 2 Batch 1450 Loss 1.708 Accuracy 0.3645\n",
            "Epoch 2 Batch 1500 Loss 1.703 Accuracy 0.3655\n",
            "Epoch 2 Batch 1550 Loss 1.699 Accuracy 0.3666\n",
            "Epoch 2 Batch 1600 Loss 1.695 Accuracy 0.3676\n",
            "Epoch 2 Batch 1650 Loss 1.69 Accuracy 0.3683\n",
            "Epoch 2 Batch 1700 Loss 1.685 Accuracy 0.3693\n",
            "Epoch 2 Batch 1750 Loss 1.681 Accuracy 0.3703\n",
            "Epoch 2 Batch 1800 Loss 1.678 Accuracy 0.3711\n",
            "Epoch 2 Batch 1850 Loss 1.674 Accuracy 0.3719\n",
            "Epoch 2 Batch 1900 Loss 1.669 Accuracy 0.3728\n",
            "Epoch 2 Batch 1950 Loss 1.665 Accuracy 0.3735\n",
            "Epoch 2 Batch 2000 Loss 1.66 Accuracy 0.3743\n",
            "Epoch 2 Batch 2050 Loss 1.655 Accuracy 0.3750\n",
            "Epoch 2 Batch 2100 Loss 1.649 Accuracy 0.3757\n",
            "Epoch 2 Batch 2150 Loss 1.642 Accuracy 0.3763\n",
            "Epoch 2 Batch 2200 Loss 1.636 Accuracy 0.3770\n",
            "Epoch 2 Batch 2250 Loss 1.631 Accuracy 0.3776\n",
            "Epoch 2 Batch 2300 Loss 1.626 Accuracy 0.3781\n",
            "Epoch 2 Batch 2350 Loss 1.62 Accuracy 0.3788\n",
            "Epoch 2 Batch 2400 Loss 1.615 Accuracy 0.3793\n",
            "Epoch 2 Batch 2450 Loss 1.61 Accuracy 0.3799\n",
            "Epoch 2 Batch 2500 Loss 1.604 Accuracy 0.3806\n",
            "Epoch 2 Batch 2550 Loss 1.599 Accuracy 0.3812\n",
            "Epoch 2 Batch 2600 Loss 1.593 Accuracy 0.3820\n",
            "Epoch 2 Batch 2650 Loss 1.588 Accuracy 0.3827\n",
            "Epoch 2 Batch 2700 Loss 1.582 Accuracy 0.3834\n",
            "Epoch 2 Batch 2750 Loss 1.577 Accuracy 0.3843\n",
            "Epoch 2 Batch 2800 Loss 1.572 Accuracy 0.3850\n",
            "Epoch 2 Batch 2850 Loss 1.568 Accuracy 0.3857\n",
            "Epoch 2 Batch 2900 Loss 1.563 Accuracy 0.3863\n",
            "Epoch 2 Batch 2950 Loss 1.559 Accuracy 0.3868\n",
            "Epoch 2 Batch 3000 Loss 1.556 Accuracy 0.3872\n",
            "Epoch 2 Batch 3050 Loss 1.552 Accuracy 0.3876\n",
            "Epoch 2 Batch 3100 Loss 1.549 Accuracy 0.3880\n",
            "Epoch 2 Batch 3150 Loss 1.546 Accuracy 0.3884\n",
            "Epoch 2 Batch 3200 Loss 1.542 Accuracy 0.3887\n",
            "Epoch 2 Batch 3250 Loss 1.539 Accuracy 0.3890\n",
            "Epoch 2 Batch 3300 Loss 1.536 Accuracy 0.3893\n",
            "Epoch 2 Batch 3350 Loss 1.532 Accuracy 0.3896\n",
            "Epoch 2 Batch 3400 Loss 1.53 Accuracy 0.3899\n",
            "Epoch 2 Batch 3450 Loss 1.527 Accuracy 0.3902\n",
            "Epoch 2 Batch 3500 Loss 1.525 Accuracy 0.3906\n",
            "Epoch 2 Batch 3550 Loss 1.522 Accuracy 0.3910\n",
            "Epoch 2 Batch 3600 Loss 1.52 Accuracy 0.3913\n",
            "Epoch 2 Batch 3650 Loss 1.517 Accuracy 0.3917\n",
            "Epoch 2 Batch 3700 Loss 1.515 Accuracy 0.3921\n",
            "Epoch 2 Batch 3750 Loss 1.512 Accuracy 0.3924\n",
            "Epoch 2 Batch 3800 Loss 1.51 Accuracy 0.3927\n",
            "Epoch 2 Batch 3850 Loss 1.508 Accuracy 0.3930\n",
            "Epoch 2 Batch 3900 Loss 1.506 Accuracy 0.3933\n",
            "Epoch 2 Batch 3950 Loss 1.504 Accuracy 0.3935\n",
            "Epoch 2 Batch 4000 Loss 1.502 Accuracy 0.3938\n",
            "Epoch 2 Batch 4050 Loss 1.501 Accuracy 0.3940\n",
            "Epoch 2 Batch 4100 Loss 1.5 Accuracy 0.3940\n",
            "Epoch 2 Batch 4150 Loss 1.5 Accuracy 0.3941\n",
            "Epoch 2 Batch 4200 Loss 1.499 Accuracy 0.3941\n",
            "Epoch 2 Batch 4250 Loss 1.499 Accuracy 0.3941\n",
            "Epoch 2 Batch 4300 Loss 1.5 Accuracy 0.3940\n",
            "Epoch 2 Batch 4350 Loss 1.501 Accuracy 0.3940\n",
            "Epoch 2 Batch 4400 Loss 1.501 Accuracy 0.3939\n",
            "Epoch 2 Batch 4450 Loss 1.502 Accuracy 0.3938\n",
            "Epoch 2 Batch 4500 Loss 1.502 Accuracy 0.3937\n",
            "Epoch 2 Batch 4550 Loss 1.503 Accuracy 0.3936\n",
            "Epoch 2 Batch 4600 Loss 1.504 Accuracy 0.3935\n",
            "Epoch 2 Batch 4650 Loss 1.504 Accuracy 0.3935\n",
            "Epoch 2 Batch 4700 Loss 1.505 Accuracy 0.3934\n",
            "Epoch 2 Batch 4750 Loss 1.505 Accuracy 0.3933\n",
            "Epoch 2 Batch 4800 Loss 1.506 Accuracy 0.3932\n",
            "Epoch 2 Batch 4850 Loss 1.506 Accuracy 0.3932\n",
            "Epoch 2 Batch 4900 Loss 1.507 Accuracy 0.3930\n",
            "Epoch 2 Batch 4950 Loss 1.507 Accuracy 0.3930\n",
            "Epoch 2 Batch 5000 Loss 1.507 Accuracy 0.3928\n",
            "Epoch 2 Batch 5050 Loss 1.508 Accuracy 0.3927\n",
            "Epoch 2 Batch 5100 Loss 1.508 Accuracy 0.3925\n",
            "Epoch 2 Batch 5150 Loss 1.509 Accuracy 0.3923\n",
            "Epoch 2 Batch 5200 Loss 1.509 Accuracy 0.3922\n",
            "Epoch 2 Batch 5250 Loss 1.509 Accuracy 0.3920\n",
            "Epoch 2 Batch 5300 Loss 1.509 Accuracy 0.3918\n",
            "Epoch 2 Batch 5350 Loss 1.51 Accuracy 0.3917\n",
            "Epoch 2 Batch 5400 Loss 1.51 Accuracy 0.3916\n",
            "Epoch 2 Batch 5450 Loss 1.51 Accuracy 0.3915\n",
            "Epoch 2 Batch 5500 Loss 1.51 Accuracy 0.3914\n",
            "Epoch 2 Batch 5550 Loss 1.51 Accuracy 0.3913\n",
            "Saving checkpoints for epoch 2 at ./Checkpoints/ckpt-2\n",
            "Time taken for 1 epoch: 1470.7265303134918 secs\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.576 Accuracy 0.3988\n",
            "Epoch 3 Batch 50 Loss 1.555 Accuracy 0.3845\n",
            "Epoch 3 Batch 100 Loss 1.555 Accuracy 0.3850\n",
            "Epoch 3 Batch 150 Loss 1.536 Accuracy 0.3867\n",
            "Epoch 3 Batch 200 Loss 1.541 Accuracy 0.3869\n",
            "Epoch 3 Batch 250 Loss 1.533 Accuracy 0.3875\n",
            "Epoch 3 Batch 300 Loss 1.529 Accuracy 0.3879\n",
            "Epoch 3 Batch 350 Loss 1.531 Accuracy 0.3878\n",
            "Epoch 3 Batch 400 Loss 1.523 Accuracy 0.3874\n",
            "Epoch 3 Batch 450 Loss 1.517 Accuracy 0.3868\n",
            "Epoch 3 Batch 500 Loss 1.513 Accuracy 0.3872\n",
            "Epoch 3 Batch 550 Loss 1.511 Accuracy 0.3875\n",
            "Epoch 3 Batch 600 Loss 1.507 Accuracy 0.3879\n",
            "Epoch 3 Batch 650 Loss 1.507 Accuracy 0.3883\n",
            "Epoch 3 Batch 700 Loss 1.504 Accuracy 0.3894\n",
            "Epoch 3 Batch 750 Loss 1.501 Accuracy 0.3903\n",
            "Epoch 3 Batch 800 Loss 1.496 Accuracy 0.3912\n",
            "Epoch 3 Batch 850 Loss 1.49 Accuracy 0.3922\n",
            "Epoch 3 Batch 900 Loss 1.483 Accuracy 0.3930\n",
            "Epoch 3 Batch 950 Loss 1.479 Accuracy 0.3935\n",
            "Epoch 3 Batch 1000 Loss 1.474 Accuracy 0.3941\n",
            "Epoch 3 Batch 1050 Loss 1.471 Accuracy 0.3946\n",
            "Epoch 3 Batch 1100 Loss 1.466 Accuracy 0.3953\n",
            "Epoch 3 Batch 1150 Loss 1.462 Accuracy 0.3959\n",
            "Epoch 3 Batch 1200 Loss 1.458 Accuracy 0.3965\n",
            "Epoch 3 Batch 1250 Loss 1.455 Accuracy 0.3974\n",
            "Epoch 3 Batch 1300 Loss 1.45 Accuracy 0.3984\n",
            "Epoch 3 Batch 1350 Loss 1.446 Accuracy 0.3994\n",
            "Epoch 3 Batch 1400 Loss 1.443 Accuracy 0.4003\n",
            "Epoch 3 Batch 1450 Loss 1.439 Accuracy 0.4013\n",
            "Epoch 3 Batch 1500 Loss 1.435 Accuracy 0.4022\n",
            "Epoch 3 Batch 1550 Loss 1.432 Accuracy 0.4030\n",
            "Epoch 3 Batch 1600 Loss 1.429 Accuracy 0.4037\n",
            "Epoch 3 Batch 1650 Loss 1.426 Accuracy 0.4044\n",
            "Epoch 3 Batch 1700 Loss 1.423 Accuracy 0.4053\n",
            "Epoch 3 Batch 1750 Loss 1.42 Accuracy 0.4061\n",
            "Epoch 3 Batch 1800 Loss 1.416 Accuracy 0.4069\n",
            "Epoch 3 Batch 1850 Loss 1.412 Accuracy 0.4076\n",
            "Epoch 3 Batch 1900 Loss 1.409 Accuracy 0.4083\n",
            "Epoch 3 Batch 1950 Loss 1.406 Accuracy 0.4089\n",
            "Epoch 3 Batch 2000 Loss 1.402 Accuracy 0.4095\n",
            "Epoch 3 Batch 2050 Loss 1.397 Accuracy 0.4099\n",
            "Epoch 3 Batch 2100 Loss 1.392 Accuracy 0.4104\n",
            "Epoch 3 Batch 2150 Loss 1.387 Accuracy 0.4110\n",
            "Epoch 3 Batch 2200 Loss 1.383 Accuracy 0.4115\n",
            "Epoch 3 Batch 2250 Loss 1.38 Accuracy 0.4119\n",
            "Epoch 3 Batch 2300 Loss 1.375 Accuracy 0.4125\n",
            "Epoch 3 Batch 2350 Loss 1.371 Accuracy 0.4130\n",
            "Epoch 3 Batch 2400 Loss 1.367 Accuracy 0.4134\n",
            "Epoch 3 Batch 2450 Loss 1.363 Accuracy 0.4139\n",
            "Epoch 3 Batch 2500 Loss 1.359 Accuracy 0.4145\n",
            "Epoch 3 Batch 2550 Loss 1.354 Accuracy 0.4150\n",
            "Epoch 3 Batch 2600 Loss 1.35 Accuracy 0.4156\n",
            "Epoch 3 Batch 2650 Loss 1.346 Accuracy 0.4164\n",
            "Epoch 3 Batch 2700 Loss 1.341 Accuracy 0.4170\n",
            "Epoch 3 Batch 2750 Loss 1.337 Accuracy 0.4177\n",
            "Epoch 3 Batch 2800 Loss 1.333 Accuracy 0.4183\n",
            "Epoch 3 Batch 2850 Loss 1.329 Accuracy 0.4187\n",
            "Epoch 3 Batch 2900 Loss 1.326 Accuracy 0.4193\n",
            "Epoch 3 Batch 2950 Loss 1.323 Accuracy 0.4197\n",
            "Epoch 3 Batch 3000 Loss 1.32 Accuracy 0.4199\n",
            "Epoch 3 Batch 3050 Loss 1.318 Accuracy 0.4202\n",
            "Epoch 3 Batch 3100 Loss 1.316 Accuracy 0.4204\n",
            "Epoch 3 Batch 3150 Loss 1.314 Accuracy 0.4206\n",
            "Epoch 3 Batch 3200 Loss 1.312 Accuracy 0.4209\n",
            "Epoch 3 Batch 3250 Loss 1.31 Accuracy 0.4210\n",
            "Epoch 3 Batch 3300 Loss 1.308 Accuracy 0.4212\n",
            "Epoch 3 Batch 3350 Loss 1.306 Accuracy 0.4214\n",
            "Epoch 3 Batch 3400 Loss 1.305 Accuracy 0.4216\n",
            "Epoch 3 Batch 3450 Loss 1.303 Accuracy 0.4218\n",
            "Epoch 3 Batch 3500 Loss 1.301 Accuracy 0.4220\n",
            "Epoch 3 Batch 3550 Loss 1.3 Accuracy 0.4221\n",
            "Epoch 3 Batch 3600 Loss 1.298 Accuracy 0.4224\n",
            "Epoch 3 Batch 3650 Loss 1.297 Accuracy 0.4226\n",
            "Epoch 3 Batch 3700 Loss 1.295 Accuracy 0.4228\n",
            "Epoch 3 Batch 3750 Loss 1.294 Accuracy 0.4230\n",
            "Epoch 3 Batch 3800 Loss 1.293 Accuracy 0.4233\n",
            "Epoch 3 Batch 3850 Loss 1.292 Accuracy 0.4234\n",
            "Epoch 3 Batch 3900 Loss 1.291 Accuracy 0.4236\n",
            "Epoch 3 Batch 3950 Loss 1.289 Accuracy 0.4238\n",
            "Epoch 3 Batch 4000 Loss 1.288 Accuracy 0.4239\n",
            "Epoch 3 Batch 4050 Loss 1.288 Accuracy 0.4240\n",
            "Epoch 3 Batch 4100 Loss 1.288 Accuracy 0.4239\n",
            "Epoch 3 Batch 4150 Loss 1.288 Accuracy 0.4238\n",
            "Epoch 3 Batch 4200 Loss 1.289 Accuracy 0.4236\n",
            "Epoch 3 Batch 4250 Loss 1.29 Accuracy 0.4235\n",
            "Epoch 3 Batch 4300 Loss 1.292 Accuracy 0.4233\n",
            "Epoch 3 Batch 4350 Loss 1.293 Accuracy 0.4231\n",
            "Epoch 3 Batch 4400 Loss 1.294 Accuracy 0.4230\n",
            "Epoch 3 Batch 4450 Loss 1.296 Accuracy 0.4228\n",
            "Epoch 3 Batch 4500 Loss 1.297 Accuracy 0.4225\n",
            "Epoch 3 Batch 4550 Loss 1.299 Accuracy 0.4223\n",
            "Epoch 3 Batch 4600 Loss 1.3 Accuracy 0.4221\n",
            "Epoch 3 Batch 4650 Loss 1.302 Accuracy 0.4219\n",
            "Epoch 3 Batch 4700 Loss 1.303 Accuracy 0.4217\n",
            "Epoch 3 Batch 4750 Loss 1.304 Accuracy 0.4215\n",
            "Epoch 3 Batch 4800 Loss 1.306 Accuracy 0.4213\n",
            "Epoch 3 Batch 4850 Loss 1.307 Accuracy 0.4211\n",
            "Epoch 3 Batch 4900 Loss 1.309 Accuracy 0.4208\n",
            "Epoch 3 Batch 4950 Loss 1.31 Accuracy 0.4206\n",
            "Epoch 3 Batch 5000 Loss 1.311 Accuracy 0.4204\n",
            "Epoch 3 Batch 5050 Loss 1.312 Accuracy 0.4201\n",
            "Epoch 3 Batch 5100 Loss 1.313 Accuracy 0.4199\n",
            "Epoch 3 Batch 5150 Loss 1.314 Accuracy 0.4196\n",
            "Epoch 3 Batch 5200 Loss 1.315 Accuracy 0.4193\n",
            "Epoch 3 Batch 5250 Loss 1.316 Accuracy 0.4191\n",
            "Epoch 3 Batch 5300 Loss 1.317 Accuracy 0.4188\n",
            "Epoch 3 Batch 5350 Loss 1.318 Accuracy 0.4186\n",
            "Epoch 3 Batch 5400 Loss 1.319 Accuracy 0.4183\n",
            "Epoch 3 Batch 5450 Loss 1.32 Accuracy 0.4181\n",
            "Epoch 3 Batch 5500 Loss 1.321 Accuracy 0.4179\n",
            "Epoch 3 Batch 5550 Loss 1.322 Accuracy 0.4177\n",
            "Saving checkpoints for epoch 3 at ./Checkpoints/ckpt-3\n",
            "Time taken for 1 epoch: 1467.4381580352783 secs\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.447 Accuracy 0.4112\n",
            "Epoch 4 Batch 50 Loss 1.45 Accuracy 0.3965\n",
            "Epoch 4 Batch 100 Loss 1.429 Accuracy 0.3981\n",
            "Epoch 4 Batch 150 Loss 1.422 Accuracy 0.4001\n",
            "Epoch 4 Batch 200 Loss 1.418 Accuracy 0.4008\n",
            "Epoch 4 Batch 250 Loss 1.415 Accuracy 0.4008\n",
            "Epoch 4 Batch 300 Loss 1.412 Accuracy 0.4013\n",
            "Epoch 4 Batch 350 Loss 1.414 Accuracy 0.4016\n",
            "Epoch 4 Batch 400 Loss 1.413 Accuracy 0.4021\n",
            "Epoch 4 Batch 450 Loss 1.412 Accuracy 0.4015\n",
            "Epoch 4 Batch 500 Loss 1.408 Accuracy 0.4019\n",
            "Epoch 4 Batch 550 Loss 1.408 Accuracy 0.4019\n",
            "Epoch 4 Batch 600 Loss 1.405 Accuracy 0.4025\n",
            "Epoch 4 Batch 650 Loss 1.401 Accuracy 0.4034\n",
            "Epoch 4 Batch 700 Loss 1.397 Accuracy 0.4043\n",
            "Epoch 4 Batch 750 Loss 1.392 Accuracy 0.4050\n",
            "Epoch 4 Batch 800 Loss 1.388 Accuracy 0.4060\n",
            "Epoch 4 Batch 850 Loss 1.381 Accuracy 0.4070\n",
            "Epoch 4 Batch 900 Loss 1.377 Accuracy 0.4079\n",
            "Epoch 4 Batch 950 Loss 1.372 Accuracy 0.4086\n",
            "Epoch 4 Batch 1000 Loss 1.368 Accuracy 0.4094\n",
            "Epoch 4 Batch 1050 Loss 1.365 Accuracy 0.4099\n",
            "Epoch 4 Batch 1100 Loss 1.361 Accuracy 0.4105\n",
            "Epoch 4 Batch 1150 Loss 1.358 Accuracy 0.4110\n",
            "Epoch 4 Batch 1200 Loss 1.354 Accuracy 0.4114\n",
            "Epoch 4 Batch 1250 Loss 1.35 Accuracy 0.4121\n",
            "Epoch 4 Batch 1300 Loss 1.346 Accuracy 0.4131\n",
            "Epoch 4 Batch 1350 Loss 1.342 Accuracy 0.4141\n",
            "Epoch 4 Batch 1400 Loss 1.338 Accuracy 0.4151\n",
            "Epoch 4 Batch 1450 Loss 1.334 Accuracy 0.4160\n",
            "Epoch 4 Batch 1500 Loss 1.331 Accuracy 0.4170\n",
            "Epoch 4 Batch 1550 Loss 1.329 Accuracy 0.4177\n",
            "Epoch 4 Batch 1600 Loss 1.325 Accuracy 0.4185\n",
            "Epoch 4 Batch 1650 Loss 1.323 Accuracy 0.4195\n",
            "Epoch 4 Batch 1700 Loss 1.32 Accuracy 0.4204\n",
            "Epoch 4 Batch 1750 Loss 1.316 Accuracy 0.4211\n",
            "Epoch 4 Batch 1800 Loss 1.314 Accuracy 0.4218\n",
            "Epoch 4 Batch 1850 Loss 1.311 Accuracy 0.4226\n",
            "Epoch 4 Batch 1900 Loss 1.307 Accuracy 0.4233\n",
            "Epoch 4 Batch 1950 Loss 1.304 Accuracy 0.4237\n",
            "Epoch 4 Batch 2000 Loss 1.3 Accuracy 0.4244\n",
            "Epoch 4 Batch 2050 Loss 1.297 Accuracy 0.4248\n",
            "Epoch 4 Batch 2100 Loss 1.292 Accuracy 0.4253\n",
            "Epoch 4 Batch 2150 Loss 1.288 Accuracy 0.4257\n",
            "Epoch 4 Batch 2200 Loss 1.283 Accuracy 0.4261\n",
            "Epoch 4 Batch 2250 Loss 1.279 Accuracy 0.4266\n",
            "Epoch 4 Batch 2300 Loss 1.275 Accuracy 0.4270\n",
            "Epoch 4 Batch 2350 Loss 1.271 Accuracy 0.4275\n",
            "Epoch 4 Batch 2400 Loss 1.267 Accuracy 0.4279\n",
            "Epoch 4 Batch 2450 Loss 1.264 Accuracy 0.4283\n",
            "Epoch 4 Batch 2500 Loss 1.26 Accuracy 0.4288\n",
            "Epoch 4 Batch 2550 Loss 1.256 Accuracy 0.4294\n",
            "Epoch 4 Batch 2600 Loss 1.252 Accuracy 0.4300\n",
            "Epoch 4 Batch 2650 Loss 1.248 Accuracy 0.4306\n",
            "Epoch 4 Batch 2700 Loss 1.244 Accuracy 0.4311\n",
            "Epoch 4 Batch 2750 Loss 1.24 Accuracy 0.4318\n",
            "Epoch 4 Batch 2800 Loss 1.236 Accuracy 0.4324\n",
            "Epoch 4 Batch 2850 Loss 1.233 Accuracy 0.4327\n",
            "Epoch 4 Batch 2900 Loss 1.23 Accuracy 0.4331\n",
            "Epoch 4 Batch 2950 Loss 1.228 Accuracy 0.4335\n",
            "Epoch 4 Batch 3000 Loss 1.225 Accuracy 0.4337\n",
            "Epoch 4 Batch 3050 Loss 1.223 Accuracy 0.4340\n",
            "Epoch 4 Batch 3100 Loss 1.221 Accuracy 0.4342\n",
            "Epoch 4 Batch 3150 Loss 1.219 Accuracy 0.4344\n",
            "Epoch 4 Batch 3200 Loss 1.217 Accuracy 0.4346\n",
            "Epoch 4 Batch 3250 Loss 1.216 Accuracy 0.4347\n",
            "Epoch 4 Batch 3300 Loss 1.215 Accuracy 0.4348\n",
            "Epoch 4 Batch 3350 Loss 1.213 Accuracy 0.4351\n",
            "Epoch 4 Batch 3400 Loss 1.212 Accuracy 0.4352\n",
            "Epoch 4 Batch 3450 Loss 1.211 Accuracy 0.4353\n",
            "Epoch 4 Batch 3500 Loss 1.209 Accuracy 0.4355\n",
            "Epoch 4 Batch 3550 Loss 1.208 Accuracy 0.4357\n",
            "Epoch 4 Batch 3600 Loss 1.207 Accuracy 0.4359\n",
            "Epoch 4 Batch 3650 Loss 1.206 Accuracy 0.4360\n",
            "Epoch 4 Batch 3700 Loss 1.205 Accuracy 0.4361\n",
            "Epoch 4 Batch 3750 Loss 1.204 Accuracy 0.4363\n",
            "Epoch 4 Batch 3800 Loss 1.203 Accuracy 0.4364\n",
            "Epoch 4 Batch 3850 Loss 1.202 Accuracy 0.4365\n",
            "Epoch 4 Batch 3900 Loss 1.201 Accuracy 0.4367\n",
            "Epoch 4 Batch 3950 Loss 1.2 Accuracy 0.4369\n",
            "Epoch 4 Batch 4000 Loss 1.199 Accuracy 0.4370\n",
            "Epoch 4 Batch 4050 Loss 1.199 Accuracy 0.4370\n",
            "Epoch 4 Batch 4100 Loss 1.199 Accuracy 0.4369\n",
            "Epoch 4 Batch 4150 Loss 1.2 Accuracy 0.4368\n",
            "Epoch 4 Batch 4200 Loss 1.201 Accuracy 0.4367\n",
            "Epoch 4 Batch 4250 Loss 1.202 Accuracy 0.4365\n",
            "Epoch 4 Batch 4300 Loss 1.203 Accuracy 0.4363\n",
            "Epoch 4 Batch 4350 Loss 1.204 Accuracy 0.4361\n",
            "Epoch 4 Batch 4400 Loss 1.206 Accuracy 0.4359\n",
            "Epoch 4 Batch 4450 Loss 1.208 Accuracy 0.4356\n",
            "Epoch 4 Batch 4500 Loss 1.209 Accuracy 0.4353\n",
            "Epoch 4 Batch 4550 Loss 1.211 Accuracy 0.4350\n",
            "Epoch 4 Batch 4600 Loss 1.212 Accuracy 0.4348\n",
            "Epoch 4 Batch 4650 Loss 1.214 Accuracy 0.4346\n",
            "Epoch 4 Batch 4700 Loss 1.216 Accuracy 0.4343\n",
            "Epoch 4 Batch 4750 Loss 1.217 Accuracy 0.4340\n",
            "Epoch 4 Batch 4800 Loss 1.219 Accuracy 0.4337\n",
            "Epoch 4 Batch 4850 Loss 1.22 Accuracy 0.4335\n",
            "Epoch 4 Batch 4900 Loss 1.222 Accuracy 0.4332\n",
            "Epoch 4 Batch 4950 Loss 1.223 Accuracy 0.4330\n",
            "Epoch 4 Batch 5000 Loss 1.224 Accuracy 0.4327\n",
            "Epoch 4 Batch 5050 Loss 1.226 Accuracy 0.4325\n",
            "Epoch 4 Batch 5100 Loss 1.227 Accuracy 0.4322\n",
            "Epoch 4 Batch 5150 Loss 1.229 Accuracy 0.4320\n",
            "Epoch 4 Batch 5200 Loss 1.23 Accuracy 0.4317\n",
            "Epoch 4 Batch 5250 Loss 1.231 Accuracy 0.4314\n",
            "Epoch 4 Batch 5300 Loss 1.233 Accuracy 0.4311\n",
            "Epoch 4 Batch 5350 Loss 1.234 Accuracy 0.4308\n",
            "Epoch 4 Batch 5400 Loss 1.235 Accuracy 0.4306\n",
            "Epoch 4 Batch 5450 Loss 1.236 Accuracy 0.4303\n",
            "Epoch 4 Batch 5500 Loss 1.237 Accuracy 0.4301\n",
            "Epoch 4 Batch 5550 Loss 1.239 Accuracy 0.4298\n",
            "Saving checkpoints for epoch 4 at ./Checkpoints/ckpt-4\n",
            "Time taken for 1 epoch: 1472.9382519721985 secs\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.508 Accuracy 0.4161\n",
            "Epoch 5 Batch 50 Loss 1.387 Accuracy 0.4081\n",
            "Epoch 5 Batch 100 Loss 1.372 Accuracy 0.4111\n",
            "Epoch 5 Batch 150 Loss 1.361 Accuracy 0.4113\n",
            "Epoch 5 Batch 200 Loss 1.358 Accuracy 0.4117\n",
            "Epoch 5 Batch 250 Loss 1.358 Accuracy 0.4107\n",
            "Epoch 5 Batch 300 Loss 1.358 Accuracy 0.4113\n",
            "Epoch 5 Batch 350 Loss 1.354 Accuracy 0.4114\n",
            "Epoch 5 Batch 400 Loss 1.35 Accuracy 0.4117\n",
            "Epoch 5 Batch 450 Loss 1.347 Accuracy 0.4120\n",
            "Epoch 5 Batch 500 Loss 1.344 Accuracy 0.4119\n",
            "Epoch 5 Batch 550 Loss 1.344 Accuracy 0.4116\n",
            "Epoch 5 Batch 600 Loss 1.341 Accuracy 0.4120\n",
            "Epoch 5 Batch 650 Loss 1.336 Accuracy 0.4123\n",
            "Epoch 5 Batch 700 Loss 1.331 Accuracy 0.4127\n",
            "Epoch 5 Batch 750 Loss 1.327 Accuracy 0.4134\n",
            "Epoch 5 Batch 800 Loss 1.325 Accuracy 0.4141\n",
            "Epoch 5 Batch 850 Loss 1.32 Accuracy 0.4151\n",
            "Epoch 5 Batch 900 Loss 1.316 Accuracy 0.4161\n",
            "Epoch 5 Batch 950 Loss 1.312 Accuracy 0.4170\n",
            "Epoch 5 Batch 1000 Loss 1.309 Accuracy 0.4175\n",
            "Epoch 5 Batch 1050 Loss 1.305 Accuracy 0.4183\n",
            "Epoch 5 Batch 1100 Loss 1.301 Accuracy 0.4189\n",
            "Epoch 5 Batch 1150 Loss 1.298 Accuracy 0.4195\n",
            "Epoch 5 Batch 1200 Loss 1.296 Accuracy 0.4202\n",
            "Epoch 5 Batch 1250 Loss 1.291 Accuracy 0.4208\n",
            "Epoch 5 Batch 1300 Loss 1.288 Accuracy 0.4218\n",
            "Epoch 5 Batch 1350 Loss 1.285 Accuracy 0.4229\n",
            "Epoch 5 Batch 1400 Loss 1.281 Accuracy 0.4238\n",
            "Epoch 5 Batch 1450 Loss 1.277 Accuracy 0.4247\n",
            "Epoch 5 Batch 1500 Loss 1.274 Accuracy 0.4255\n",
            "Epoch 5 Batch 1550 Loss 1.27 Accuracy 0.4262\n",
            "Epoch 5 Batch 1600 Loss 1.268 Accuracy 0.4272\n",
            "Epoch 5 Batch 1650 Loss 1.265 Accuracy 0.4279\n",
            "Epoch 5 Batch 1700 Loss 1.262 Accuracy 0.4288\n",
            "Epoch 5 Batch 1750 Loss 1.259 Accuracy 0.4296\n",
            "Epoch 5 Batch 1800 Loss 1.257 Accuracy 0.4305\n",
            "Epoch 5 Batch 1850 Loss 1.254 Accuracy 0.4313\n",
            "Epoch 5 Batch 1900 Loss 1.25 Accuracy 0.4320\n",
            "Epoch 5 Batch 1950 Loss 1.247 Accuracy 0.4326\n",
            "Epoch 5 Batch 2000 Loss 1.243 Accuracy 0.4331\n",
            "Epoch 5 Batch 2050 Loss 1.239 Accuracy 0.4335\n",
            "Epoch 5 Batch 2100 Loss 1.235 Accuracy 0.4340\n",
            "Epoch 5 Batch 2150 Loss 1.231 Accuracy 0.4345\n",
            "Epoch 5 Batch 2200 Loss 1.226 Accuracy 0.4349\n",
            "Epoch 5 Batch 2250 Loss 1.222 Accuracy 0.4351\n",
            "Epoch 5 Batch 2300 Loss 1.218 Accuracy 0.4356\n",
            "Epoch 5 Batch 2350 Loss 1.214 Accuracy 0.4361\n",
            "Epoch 5 Batch 2400 Loss 1.211 Accuracy 0.4366\n",
            "Epoch 5 Batch 2450 Loss 1.207 Accuracy 0.4370\n",
            "Epoch 5 Batch 2500 Loss 1.203 Accuracy 0.4375\n",
            "Epoch 5 Batch 2550 Loss 1.2 Accuracy 0.4381\n",
            "Epoch 5 Batch 2600 Loss 1.196 Accuracy 0.4387\n",
            "Epoch 5 Batch 2650 Loss 1.193 Accuracy 0.4392\n",
            "Epoch 5 Batch 2700 Loss 1.189 Accuracy 0.4398\n",
            "Epoch 5 Batch 2750 Loss 1.185 Accuracy 0.4403\n",
            "Epoch 5 Batch 2800 Loss 1.181 Accuracy 0.4408\n",
            "Epoch 5 Batch 2850 Loss 1.178 Accuracy 0.4412\n",
            "Epoch 5 Batch 2900 Loss 1.175 Accuracy 0.4416\n",
            "Epoch 5 Batch 2950 Loss 1.173 Accuracy 0.4419\n",
            "Epoch 5 Batch 3000 Loss 1.17 Accuracy 0.4422\n",
            "Epoch 5 Batch 3050 Loss 1.168 Accuracy 0.4425\n",
            "Epoch 5 Batch 3100 Loss 1.166 Accuracy 0.4426\n",
            "Epoch 5 Batch 3150 Loss 1.165 Accuracy 0.4428\n",
            "Epoch 5 Batch 3200 Loss 1.163 Accuracy 0.4429\n",
            "Epoch 5 Batch 3250 Loss 1.161 Accuracy 0.4430\n",
            "Epoch 5 Batch 3300 Loss 1.16 Accuracy 0.4432\n",
            "Epoch 5 Batch 3350 Loss 1.158 Accuracy 0.4433\n",
            "Epoch 5 Batch 3400 Loss 1.157 Accuracy 0.4434\n",
            "Epoch 5 Batch 3450 Loss 1.156 Accuracy 0.4435\n",
            "Epoch 5 Batch 3500 Loss 1.155 Accuracy 0.4437\n",
            "Epoch 5 Batch 3550 Loss 1.153 Accuracy 0.4438\n",
            "Epoch 5 Batch 3600 Loss 1.153 Accuracy 0.4439\n",
            "Epoch 5 Batch 3650 Loss 1.152 Accuracy 0.4441\n",
            "Epoch 5 Batch 3700 Loss 1.151 Accuracy 0.4442\n",
            "Epoch 5 Batch 3750 Loss 1.149 Accuracy 0.4444\n",
            "Epoch 5 Batch 3800 Loss 1.148 Accuracy 0.4445\n",
            "Epoch 5 Batch 3850 Loss 1.148 Accuracy 0.4446\n",
            "Epoch 5 Batch 3900 Loss 1.147 Accuracy 0.4448\n",
            "Epoch 5 Batch 3950 Loss 1.146 Accuracy 0.4448\n",
            "Epoch 5 Batch 4000 Loss 1.145 Accuracy 0.4450\n",
            "Epoch 5 Batch 4050 Loss 1.145 Accuracy 0.4450\n",
            "Epoch 5 Batch 4100 Loss 1.146 Accuracy 0.4449\n",
            "Epoch 5 Batch 4150 Loss 1.147 Accuracy 0.4447\n",
            "Epoch 5 Batch 4200 Loss 1.148 Accuracy 0.4445\n",
            "Epoch 5 Batch 4250 Loss 1.149 Accuracy 0.4443\n",
            "Epoch 5 Batch 4300 Loss 1.151 Accuracy 0.4441\n",
            "Epoch 5 Batch 4350 Loss 1.152 Accuracy 0.4438\n",
            "Epoch 5 Batch 4400 Loss 1.154 Accuracy 0.4436\n",
            "Epoch 5 Batch 4450 Loss 1.155 Accuracy 0.4433\n",
            "Epoch 5 Batch 4500 Loss 1.157 Accuracy 0.4431\n",
            "Epoch 5 Batch 4550 Loss 1.159 Accuracy 0.4428\n",
            "Epoch 5 Batch 4600 Loss 1.161 Accuracy 0.4426\n",
            "Epoch 5 Batch 4650 Loss 1.163 Accuracy 0.4423\n",
            "Epoch 5 Batch 4700 Loss 1.165 Accuracy 0.4420\n",
            "Epoch 5 Batch 4750 Loss 1.167 Accuracy 0.4418\n",
            "Epoch 5 Batch 4800 Loss 1.168 Accuracy 0.4415\n",
            "Epoch 5 Batch 4850 Loss 1.17 Accuracy 0.4413\n",
            "Epoch 5 Batch 4900 Loss 1.171 Accuracy 0.4410\n",
            "Epoch 5 Batch 4950 Loss 1.173 Accuracy 0.4407\n",
            "Epoch 5 Batch 5000 Loss 1.174 Accuracy 0.4405\n",
            "Epoch 5 Batch 5050 Loss 1.176 Accuracy 0.4402\n",
            "Epoch 5 Batch 5100 Loss 1.177 Accuracy 0.4400\n",
            "Epoch 5 Batch 5150 Loss 1.179 Accuracy 0.4397\n",
            "Epoch 5 Batch 5200 Loss 1.18 Accuracy 0.4394\n",
            "Epoch 5 Batch 5250 Loss 1.182 Accuracy 0.4391\n",
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 5.845 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.109 Accuracy 0.0119\n",
            "Epoch 1 Batch 100 Loss 6.041 Accuracy 0.0321\n",
            "Epoch 1 Batch 150 Loss 5.964 Accuracy 0.0389\n",
            "Epoch 1 Batch 200 Loss 5.893 Accuracy 0.0423\n",
            "Epoch 1 Batch 250 Loss 5.794 Accuracy 0.0444\n",
            "Epoch 1 Batch 300 Loss 5.689 Accuracy 0.0457\n",
            "Epoch 1 Batch 350 Loss 5.579 Accuracy 0.0469\n",
            "Epoch 1 Batch 400 Loss 5.468 Accuracy 0.0507\n",
            "Epoch 1 Batch 450 Loss 5.359 Accuracy 0.0552\n",
            "Epoch 1 Batch 500 Loss 5.26 Accuracy 0.0591\n",
            "Epoch 1 Batch 550 Loss 5.174 Accuracy 0.0627\n",
            "Epoch 1 Batch 600 Loss 5.093 Accuracy 0.0664\n",
            "Epoch 1 Batch 650 Loss 5.017 Accuracy 0.0702\n",
            "Epoch 1 Batch 700 Loss 4.947 Accuracy 0.0744\n",
            "Epoch 1 Batch 750 Loss 4.882 Accuracy 0.0785\n",
            "Epoch 1 Batch 800 Loss 4.817 Accuracy 0.0825\n",
            "Epoch 1 Batch 850 Loss 4.754 Accuracy 0.0865\n",
            "Epoch 1 Batch 900 Loss 4.696 Accuracy 0.0903\n",
            "Epoch 1 Batch 950 Loss 4.636 Accuracy 0.0941\n",
            "Epoch 1 Batch 1000 Loss 4.58 Accuracy 0.0977\n",
            "Epoch 1 Batch 1050 Loss 4.527 Accuracy 0.1012\n",
            "Epoch 1 Batch 1100 Loss 4.479 Accuracy 0.1045\n",
            "Epoch 1 Batch 1150 Loss 4.431 Accuracy 0.1077\n",
            "Epoch 1 Batch 1200 Loss 4.386 Accuracy 0.1107\n",
            "Epoch 1 Batch 1250 Loss 4.342 Accuracy 0.1138\n",
            "Epoch 1 Batch 1300 Loss 4.299 Accuracy 0.1168\n",
            "Epoch 1 Batch 1350 Loss 4.258 Accuracy 0.1199\n",
            "Epoch 1 Batch 1400 Loss 4.219 Accuracy 0.1229\n",
            "Epoch 1 Batch 1450 Loss 4.182 Accuracy 0.1259\n",
            "Epoch 1 Batch 1500 Loss 4.144 Accuracy 0.1288\n",
            "Epoch 1 Batch 1550 Loss 4.109 Accuracy 0.1316\n",
            "Epoch 1 Batch 1600 Loss 4.073 Accuracy 0.1345\n",
            "Epoch 1 Batch 1650 Loss 4.04 Accuracy 0.1372\n",
            "Epoch 1 Batch 1700 Loss 4.007 Accuracy 0.1398\n",
            "Epoch 1 Batch 1750 Loss 3.975 Accuracy 0.1424\n",
            "Epoch 1 Batch 1800 Loss 3.944 Accuracy 0.1450\n",
            "Epoch 1 Batch 1850 Loss 3.914 Accuracy 0.1476\n",
            "Epoch 1 Batch 1900 Loss 3.884 Accuracy 0.1501\n",
            "Epoch 1 Batch 1950 Loss 3.854 Accuracy 0.1524\n",
            "Epoch 1 Batch 2000 Loss 3.825 Accuracy 0.1547\n",
            "Epoch 1 Batch 2050 Loss 3.796 Accuracy 0.1570\n",
            "Epoch 1 Batch 2100 Loss 3.767 Accuracy 0.1593\n",
            "Epoch 1 Batch 2150 Loss 3.738 Accuracy 0.1616\n",
            "Epoch 1 Batch 2200 Loss 3.709 Accuracy 0.1638\n",
            "Epoch 1 Batch 2250 Loss 3.681 Accuracy 0.1662\n",
            "Epoch 1 Batch 2300 Loss 3.654 Accuracy 0.1684\n",
            "Epoch 1 Batch 2350 Loss 3.627 Accuracy 0.1706\n",
            "Epoch 1 Batch 2400 Loss 3.6 Accuracy 0.1728\n",
            "Epoch 1 Batch 2450 Loss 3.575 Accuracy 0.1751\n",
            "Epoch 1 Batch 2500 Loss 3.55 Accuracy 0.1774\n",
            "Epoch 1 Batch 2550 Loss 3.526 Accuracy 0.1796\n",
            "Epoch 1 Batch 2600 Loss 3.501 Accuracy 0.1818\n",
            "Epoch 1 Batch 2650 Loss 3.477 Accuracy 0.1841\n",
            "Epoch 1 Batch 2700 Loss 3.453 Accuracy 0.1864\n",
            "Epoch 1 Batch 2750 Loss 3.43 Accuracy 0.1888\n",
            "Epoch 1 Batch 2800 Loss 3.407 Accuracy 0.1909\n",
            "Epoch 1 Batch 2850 Loss 3.385 Accuracy 0.1931\n",
            "Epoch 1 Batch 2900 Loss 3.363 Accuracy 0.1951\n",
            "Epoch 1 Batch 2950 Loss 3.341 Accuracy 0.1971\n",
            "Epoch 1 Batch 3000 Loss 3.321 Accuracy 0.1992\n",
            "Epoch 1 Batch 3050 Loss 3.3 Accuracy 0.2011\n",
            "Epoch 1 Batch 3100 Loss 3.281 Accuracy 0.2030\n",
            "Epoch 1 Batch 3150 Loss 3.261 Accuracy 0.2048\n",
            "Epoch 1 Batch 3200 Loss 3.241 Accuracy 0.2067\n",
            "Epoch 1 Batch 3250 Loss 3.223 Accuracy 0.2085\n",
            "Epoch 1 Batch 3300 Loss 3.204 Accuracy 0.2102\n",
            "Epoch 1 Batch 3350 Loss 3.186 Accuracy 0.2119\n",
            "Epoch 1 Batch 3400 Loss 3.169 Accuracy 0.2136\n",
            "Epoch 1 Batch 3450 Loss 3.152 Accuracy 0.2153\n",
            "Epoch 1 Batch 3500 Loss 3.135 Accuracy 0.2170\n",
            "Epoch 1 Batch 3550 Loss 3.119 Accuracy 0.2186\n",
            "Epoch 1 Batch 3600 Loss 3.101 Accuracy 0.2203\n",
            "Epoch 1 Batch 3650 Loss 3.086 Accuracy 0.2219\n",
            "Epoch 1 Batch 3700 Loss 3.07 Accuracy 0.2236\n",
            "Epoch 1 Batch 3750 Loss 3.055 Accuracy 0.2251\n",
            "Epoch 1 Batch 3800 Loss 3.039 Accuracy 0.2266\n",
            "Epoch 1 Batch 3850 Loss 3.024 Accuracy 0.2282\n",
            "Epoch 1 Batch 3900 Loss 3.01 Accuracy 0.2297\n",
            "Epoch 1 Batch 3950 Loss 2.995 Accuracy 0.2312\n",
            "Epoch 1 Batch 4000 Loss 2.981 Accuracy 0.2326\n",
            "Epoch 1 Batch 4050 Loss 2.968 Accuracy 0.2339\n",
            "Epoch 1 Batch 4100 Loss 2.956 Accuracy 0.2351\n",
            "Epoch 1 Batch 4150 Loss 2.944 Accuracy 0.2363\n",
            "Epoch 1 Batch 4200 Loss 2.933 Accuracy 0.2374\n",
            "Epoch 1 Batch 4250 Loss 2.922 Accuracy 0.2385\n",
            "Epoch 1 Batch 4300 Loss 2.911 Accuracy 0.2396\n",
            "Epoch 1 Batch 4350 Loss 2.901 Accuracy 0.2406\n",
            "Epoch 1 Batch 4400 Loss 2.89 Accuracy 0.2416\n",
            "Epoch 1 Batch 4450 Loss 2.881 Accuracy 0.2425\n",
            "Epoch 1 Batch 4500 Loss 2.871 Accuracy 0.2435\n",
            "Epoch 1 Batch 4550 Loss 2.861 Accuracy 0.2444\n",
            "Epoch 1 Batch 4600 Loss 2.851 Accuracy 0.2454\n",
            "Epoch 1 Batch 4650 Loss 2.842 Accuracy 0.2463\n",
            "Epoch 1 Batch 4700 Loss 2.832 Accuracy 0.2472\n",
            "Epoch 1 Batch 4750 Loss 2.823 Accuracy 0.2481\n",
            "Epoch 1 Batch 4800 Loss 2.814 Accuracy 0.2490\n",
            "Epoch 1 Batch 4850 Loss 2.804 Accuracy 0.2499\n",
            "Epoch 1 Batch 4900 Loss 2.796 Accuracy 0.2508\n",
            "Epoch 1 Batch 4950 Loss 2.787 Accuracy 0.2517\n",
            "Epoch 1 Batch 5000 Loss 2.778 Accuracy 0.2525\n",
            "Epoch 1 Batch 5050 Loss 2.77 Accuracy 0.2533\n",
            "Epoch 1 Batch 5100 Loss 2.761 Accuracy 0.2541\n",
            "Epoch 1 Batch 5150 Loss 2.752 Accuracy 0.2549\n",
            "Epoch 1 Batch 5200 Loss 2.744 Accuracy 0.2557\n",
            "Epoch 1 Batch 5250 Loss 2.735 Accuracy 0.2564\n",
            "Epoch 1 Batch 5300 Loss 2.727 Accuracy 0.2571\n",
            "Epoch 1 Batch 5350 Loss 2.719 Accuracy 0.2579\n",
            "Epoch 1 Batch 5400 Loss 2.711 Accuracy 0.2586\n",
            "Epoch 1 Batch 5450 Loss 2.702 Accuracy 0.2594\n",
            "Epoch 1 Batch 5500 Loss 2.694 Accuracy 0.2601\n",
            "Epoch 1 Batch 5550 Loss 2.686 Accuracy 0.2608\n",
            "Saving checkpoints for epoch 1 at ./Checkpoints/ckpt-1\n",
            "Time taken for 1 epoch: 1487.2306196689606 secs\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 2.023 Accuracy 0.3512\n",
            "Epoch 2 Batch 50 Loss 1.851 Accuracy 0.3471\n",
            "Epoch 2 Batch 100 Loss 1.846 Accuracy 0.3455\n",
            "Epoch 2 Batch 150 Loss 1.836 Accuracy 0.3457\n",
            "Epoch 2 Batch 200 Loss 1.832 Accuracy 0.3467\n",
            "Epoch 2 Batch 250 Loss 1.822 Accuracy 0.3471\n",
            "Epoch 2 Batch 300 Loss 1.819 Accuracy 0.3468\n",
            "Epoch 2 Batch 350 Loss 1.816 Accuracy 0.3469\n",
            "Epoch 2 Batch 400 Loss 1.808 Accuracy 0.3477\n",
            "Epoch 2 Batch 450 Loss 1.806 Accuracy 0.3480\n",
            "Epoch 2 Batch 500 Loss 1.803 Accuracy 0.3484\n",
            "Epoch 2 Batch 550 Loss 1.801 Accuracy 0.3483\n",
            "Epoch 2 Batch 600 Loss 1.799 Accuracy 0.3489\n",
            "Epoch 2 Batch 650 Loss 1.793 Accuracy 0.3498\n",
            "Epoch 2 Batch 700 Loss 1.79 Accuracy 0.3505\n",
            "Epoch 2 Batch 750 Loss 1.785 Accuracy 0.3516\n",
            "Epoch 2 Batch 800 Loss 1.779 Accuracy 0.3524\n",
            "Epoch 2 Batch 850 Loss 1.773 Accuracy 0.3534\n",
            "Epoch 2 Batch 900 Loss 1.767 Accuracy 0.3544\n",
            "Epoch 2 Batch 950 Loss 1.762 Accuracy 0.3553\n",
            "Epoch 2 Batch 1000 Loss 1.757 Accuracy 0.3561\n",
            "Epoch 2 Batch 1050 Loss 1.75 Accuracy 0.3568\n",
            "Epoch 2 Batch 1100 Loss 1.743 Accuracy 0.3577\n",
            "Epoch 2 Batch 1150 Loss 1.738 Accuracy 0.3585\n",
            "Epoch 2 Batch 1200 Loss 1.734 Accuracy 0.3595\n",
            "Epoch 2 Batch 1250 Loss 1.73 Accuracy 0.3605\n",
            "Epoch 2 Batch 1300 Loss 1.724 Accuracy 0.3614\n",
            "Epoch 2 Batch 1350 Loss 1.721 Accuracy 0.3625\n",
            "Epoch 2 Batch 1400 Loss 1.715 Accuracy 0.3634\n",
            "Epoch 2 Batch 1450 Loss 1.708 Accuracy 0.3645\n",
            "Epoch 2 Batch 1500 Loss 1.703 Accuracy 0.3655\n",
            "Epoch 2 Batch 1550 Loss 1.699 Accuracy 0.3666\n",
            "Epoch 2 Batch 1600 Loss 1.695 Accuracy 0.3676\n",
            "Epoch 2 Batch 1650 Loss 1.69 Accuracy 0.3683\n",
            "Epoch 2 Batch 1700 Loss 1.685 Accuracy 0.3693\n",
            "Epoch 2 Batch 1750 Loss 1.681 Accuracy 0.3703\n",
            "Epoch 2 Batch 1800 Loss 1.678 Accuracy 0.3711\n",
            "Epoch 2 Batch 1850 Loss 1.674 Accuracy 0.3719\n",
            "Epoch 2 Batch 1900 Loss 1.669 Accuracy 0.3728\n",
            "Epoch 2 Batch 1950 Loss 1.665 Accuracy 0.3735\n",
            "Epoch 2 Batch 2000 Loss 1.66 Accuracy 0.3743\n",
            "Epoch 2 Batch 2050 Loss 1.655 Accuracy 0.3750\n",
            "Epoch 2 Batch 2100 Loss 1.649 Accuracy 0.3757\n",
            "Epoch 2 Batch 2150 Loss 1.642 Accuracy 0.3763\n",
            "Epoch 2 Batch 2200 Loss 1.636 Accuracy 0.3770\n",
            "Epoch 2 Batch 2250 Loss 1.631 Accuracy 0.3776\n",
            "Epoch 2 Batch 2300 Loss 1.626 Accuracy 0.3781\n",
            "Epoch 2 Batch 2350 Loss 1.62 Accuracy 0.3788\n",
            "Epoch 2 Batch 2400 Loss 1.615 Accuracy 0.3793\n",
            "Epoch 2 Batch 2450 Loss 1.61 Accuracy 0.3799\n",
            "Epoch 2 Batch 2500 Loss 1.604 Accuracy 0.3806\n",
            "Epoch 2 Batch 2550 Loss 1.599 Accuracy 0.3812\n",
            "Epoch 2 Batch 2600 Loss 1.593 Accuracy 0.3820\n",
            "Epoch 2 Batch 2650 Loss 1.588 Accuracy 0.3827\n",
            "Epoch 2 Batch 2700 Loss 1.582 Accuracy 0.3834\n",
            "Epoch 2 Batch 2750 Loss 1.577 Accuracy 0.3843\n",
            "Epoch 2 Batch 2800 Loss 1.572 Accuracy 0.3850\n",
            "Epoch 2 Batch 2850 Loss 1.568 Accuracy 0.3857\n",
            "Epoch 2 Batch 2900 Loss 1.563 Accuracy 0.3863\n",
            "Epoch 2 Batch 2950 Loss 1.559 Accuracy 0.3868\n",
            "Epoch 2 Batch 3000 Loss 1.556 Accuracy 0.3872\n",
            "Epoch 2 Batch 3050 Loss 1.552 Accuracy 0.3876\n",
            "Epoch 2 Batch 3100 Loss 1.549 Accuracy 0.3880\n",
            "Epoch 2 Batch 3150 Loss 1.546 Accuracy 0.3884\n",
            "Epoch 2 Batch 3200 Loss 1.542 Accuracy 0.3887\n",
            "Epoch 2 Batch 3250 Loss 1.539 Accuracy 0.3890\n",
            "Epoch 2 Batch 3300 Loss 1.536 Accuracy 0.3893\n",
            "Epoch 2 Batch 3350 Loss 1.532 Accuracy 0.3896\n",
            "Epoch 2 Batch 3400 Loss 1.53 Accuracy 0.3899\n",
            "Epoch 2 Batch 3450 Loss 1.527 Accuracy 0.3902\n",
            "Epoch 2 Batch 3500 Loss 1.525 Accuracy 0.3906\n",
            "Epoch 2 Batch 3550 Loss 1.522 Accuracy 0.3910\n",
            "Epoch 2 Batch 3600 Loss 1.52 Accuracy 0.3913\n",
            "Epoch 2 Batch 3650 Loss 1.517 Accuracy 0.3917\n",
            "Epoch 2 Batch 3700 Loss 1.515 Accuracy 0.3921\n",
            "Epoch 2 Batch 3750 Loss 1.512 Accuracy 0.3924\n",
            "Epoch 2 Batch 3800 Loss 1.51 Accuracy 0.3927\n",
            "Epoch 2 Batch 3850 Loss 1.508 Accuracy 0.3930\n",
            "Epoch 2 Batch 3900 Loss 1.506 Accuracy 0.3933\n",
            "Epoch 2 Batch 3950 Loss 1.504 Accuracy 0.3935\n",
            "Epoch 2 Batch 4000 Loss 1.502 Accuracy 0.3938\n",
            "Epoch 2 Batch 4050 Loss 1.501 Accuracy 0.3940\n",
            "Epoch 2 Batch 4100 Loss 1.5 Accuracy 0.3940\n",
            "Epoch 2 Batch 4150 Loss 1.5 Accuracy 0.3941\n",
            "Epoch 2 Batch 4200 Loss 1.499 Accuracy 0.3941\n",
            "Epoch 2 Batch 4250 Loss 1.499 Accuracy 0.3941\n",
            "Epoch 2 Batch 4300 Loss 1.5 Accuracy 0.3940\n",
            "Epoch 2 Batch 4350 Loss 1.501 Accuracy 0.3940\n",
            "Epoch 2 Batch 4400 Loss 1.501 Accuracy 0.3939\n",
            "Epoch 2 Batch 4450 Loss 1.502 Accuracy 0.3938\n",
            "Epoch 2 Batch 4500 Loss 1.502 Accuracy 0.3937\n",
            "Epoch 2 Batch 4550 Loss 1.503 Accuracy 0.3936\n",
            "Epoch 2 Batch 4600 Loss 1.504 Accuracy 0.3935\n",
            "Epoch 2 Batch 4650 Loss 1.504 Accuracy 0.3935\n",
            "Epoch 2 Batch 4700 Loss 1.505 Accuracy 0.3934\n",
            "Epoch 2 Batch 4750 Loss 1.505 Accuracy 0.3933\n",
            "Epoch 2 Batch 4800 Loss 1.506 Accuracy 0.3932\n",
            "Epoch 2 Batch 4850 Loss 1.506 Accuracy 0.3932\n",
            "Epoch 2 Batch 4900 Loss 1.507 Accuracy 0.3930\n",
            "Epoch 2 Batch 4950 Loss 1.507 Accuracy 0.3930\n",
            "Epoch 2 Batch 5000 Loss 1.507 Accuracy 0.3928\n",
            "Epoch 2 Batch 5050 Loss 1.508 Accuracy 0.3927\n",
            "Epoch 2 Batch 5100 Loss 1.508 Accuracy 0.3925\n",
            "Epoch 2 Batch 5150 Loss 1.509 Accuracy 0.3923\n",
            "Epoch 2 Batch 5200 Loss 1.509 Accuracy 0.3922\n",
            "Epoch 2 Batch 5250 Loss 1.509 Accuracy 0.3920\n",
            "Epoch 2 Batch 5300 Loss 1.509 Accuracy 0.3918\n",
            "Epoch 2 Batch 5350 Loss 1.51 Accuracy 0.3917\n",
            "Epoch 2 Batch 5400 Loss 1.51 Accuracy 0.3916\n",
            "Epoch 2 Batch 5450 Loss 1.51 Accuracy 0.3915\n",
            "Epoch 2 Batch 5500 Loss 1.51 Accuracy 0.3914\n",
            "Epoch 2 Batch 5550 Loss 1.51 Accuracy 0.3913\n",
            "Saving checkpoints for epoch 2 at ./Checkpoints/ckpt-2\n",
            "Time taken for 1 epoch: 1470.7265303134918 secs\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.576 Accuracy 0.3988\n",
            "Epoch 3 Batch 50 Loss 1.555 Accuracy 0.3845\n",
            "Epoch 3 Batch 100 Loss 1.555 Accuracy 0.3850\n",
            "Epoch 3 Batch 150 Loss 1.536 Accuracy 0.3867\n",
            "Epoch 3 Batch 200 Loss 1.541 Accuracy 0.3869\n",
            "Epoch 3 Batch 250 Loss 1.533 Accuracy 0.3875\n",
            "Epoch 3 Batch 300 Loss 1.529 Accuracy 0.3879\n",
            "Epoch 3 Batch 350 Loss 1.531 Accuracy 0.3878\n",
            "Epoch 3 Batch 400 Loss 1.523 Accuracy 0.3874\n",
            "Epoch 3 Batch 450 Loss 1.517 Accuracy 0.3868\n",
            "Epoch 3 Batch 500 Loss 1.513 Accuracy 0.3872\n",
            "Epoch 3 Batch 550 Loss 1.511 Accuracy 0.3875\n",
            "Epoch 3 Batch 600 Loss 1.507 Accuracy 0.3879\n",
            "Epoch 3 Batch 650 Loss 1.507 Accuracy 0.3883\n",
            "Epoch 3 Batch 700 Loss 1.504 Accuracy 0.3894\n",
            "Epoch 3 Batch 750 Loss 1.501 Accuracy 0.3903\n",
            "Epoch 3 Batch 800 Loss 1.496 Accuracy 0.3912\n",
            "Epoch 3 Batch 850 Loss 1.49 Accuracy 0.3922\n",
            "Epoch 3 Batch 900 Loss 1.483 Accuracy 0.3930\n",
            "Epoch 3 Batch 950 Loss 1.479 Accuracy 0.3935\n",
            "Epoch 3 Batch 1000 Loss 1.474 Accuracy 0.3941\n",
            "Epoch 3 Batch 1050 Loss 1.471 Accuracy 0.3946\n",
            "Epoch 3 Batch 1100 Loss 1.466 Accuracy 0.3953\n",
            "Epoch 3 Batch 1150 Loss 1.462 Accuracy 0.3959\n",
            "Epoch 3 Batch 1200 Loss 1.458 Accuracy 0.3965\n",
            "Epoch 3 Batch 1250 Loss 1.455 Accuracy 0.3974\n",
            "Epoch 3 Batch 1300 Loss 1.45 Accuracy 0.3984\n",
            "Epoch 3 Batch 1350 Loss 1.446 Accuracy 0.3994\n",
            "Epoch 3 Batch 1400 Loss 1.443 Accuracy 0.4003\n",
            "Epoch 3 Batch 1450 Loss 1.439 Accuracy 0.4013\n",
            "Epoch 3 Batch 1500 Loss 1.435 Accuracy 0.4022\n",
            "Epoch 3 Batch 1550 Loss 1.432 Accuracy 0.4030\n",
            "Epoch 3 Batch 1600 Loss 1.429 Accuracy 0.4037\n",
            "Epoch 3 Batch 1650 Loss 1.426 Accuracy 0.4044\n",
            "Epoch 3 Batch 1700 Loss 1.423 Accuracy 0.4053\n",
            "Epoch 3 Batch 1750 Loss 1.42 Accuracy 0.4061\n",
            "Epoch 3 Batch 1800 Loss 1.416 Accuracy 0.4069\n",
            "Epoch 3 Batch 1850 Loss 1.412 Accuracy 0.4076\n",
            "Epoch 3 Batch 1900 Loss 1.409 Accuracy 0.4083\n",
            "Epoch 3 Batch 1950 Loss 1.406 Accuracy 0.4089\n",
            "Epoch 3 Batch 2000 Loss 1.402 Accuracy 0.4095\n",
            "Epoch 3 Batch 2050 Loss 1.397 Accuracy 0.4099\n",
            "Epoch 3 Batch 2100 Loss 1.392 Accuracy 0.4104\n",
            "Epoch 3 Batch 2150 Loss 1.387 Accuracy 0.4110\n",
            "Epoch 3 Batch 2200 Loss 1.383 Accuracy 0.4115\n",
            "Epoch 3 Batch 2250 Loss 1.38 Accuracy 0.4119\n",
            "Epoch 3 Batch 2300 Loss 1.375 Accuracy 0.4125\n",
            "Epoch 3 Batch 2350 Loss 1.371 Accuracy 0.4130\n",
            "Epoch 3 Batch 2400 Loss 1.367 Accuracy 0.4134\n",
            "Epoch 3 Batch 2450 Loss 1.363 Accuracy 0.4139\n",
            "Epoch 3 Batch 2500 Loss 1.359 Accuracy 0.4145\n",
            "Epoch 3 Batch 2550 Loss 1.354 Accuracy 0.4150\n",
            "Epoch 3 Batch 2600 Loss 1.35 Accuracy 0.4156\n",
            "Epoch 3 Batch 2650 Loss 1.346 Accuracy 0.4164\n",
            "Epoch 3 Batch 2700 Loss 1.341 Accuracy 0.4170\n",
            "Epoch 3 Batch 2750 Loss 1.337 Accuracy 0.4177\n",
            "Epoch 3 Batch 2800 Loss 1.333 Accuracy 0.4183\n",
            "Epoch 3 Batch 2850 Loss 1.329 Accuracy 0.4187\n",
            "Epoch 3 Batch 2900 Loss 1.326 Accuracy 0.4193\n",
            "Epoch 3 Batch 2950 Loss 1.323 Accuracy 0.4197\n",
            "Epoch 3 Batch 3000 Loss 1.32 Accuracy 0.4199\n",
            "Epoch 3 Batch 3050 Loss 1.318 Accuracy 0.4202\n",
            "Epoch 3 Batch 3100 Loss 1.316 Accuracy 0.4204\n",
            "Epoch 3 Batch 3150 Loss 1.314 Accuracy 0.4206\n",
            "Epoch 3 Batch 3200 Loss 1.312 Accuracy 0.4209\n",
            "Epoch 3 Batch 3250 Loss 1.31 Accuracy 0.4210\n",
            "Epoch 3 Batch 3300 Loss 1.308 Accuracy 0.4212\n",
            "Epoch 3 Batch 3350 Loss 1.306 Accuracy 0.4214\n",
            "Epoch 3 Batch 3400 Loss 1.305 Accuracy 0.4216\n",
            "Epoch 3 Batch 3450 Loss 1.303 Accuracy 0.4218\n",
            "Epoch 3 Batch 3500 Loss 1.301 Accuracy 0.4220\n",
            "Epoch 3 Batch 3550 Loss 1.3 Accuracy 0.4221\n",
            "Epoch 3 Batch 3600 Loss 1.298 Accuracy 0.4224\n",
            "Epoch 3 Batch 3650 Loss 1.297 Accuracy 0.4226\n",
            "Epoch 3 Batch 3700 Loss 1.295 Accuracy 0.4228\n",
            "Epoch 3 Batch 3750 Loss 1.294 Accuracy 0.4230\n",
            "Epoch 3 Batch 3800 Loss 1.293 Accuracy 0.4233\n",
            "Epoch 3 Batch 3850 Loss 1.292 Accuracy 0.4234\n",
            "Epoch 3 Batch 3900 Loss 1.291 Accuracy 0.4236\n",
            "Epoch 3 Batch 3950 Loss 1.289 Accuracy 0.4238\n",
            "Epoch 3 Batch 4000 Loss 1.288 Accuracy 0.4239\n",
            "Epoch 3 Batch 4050 Loss 1.288 Accuracy 0.4240\n",
            "Epoch 3 Batch 4100 Loss 1.288 Accuracy 0.4239\n",
            "Epoch 3 Batch 4150 Loss 1.288 Accuracy 0.4238\n",
            "Epoch 3 Batch 4200 Loss 1.289 Accuracy 0.4236\n",
            "Epoch 3 Batch 4250 Loss 1.29 Accuracy 0.4235\n",
            "Epoch 3 Batch 4300 Loss 1.292 Accuracy 0.4233\n",
            "Epoch 3 Batch 4350 Loss 1.293 Accuracy 0.4231\n",
            "Epoch 3 Batch 4400 Loss 1.294 Accuracy 0.4230\n",
            "Epoch 3 Batch 4450 Loss 1.296 Accuracy 0.4228\n",
            "Epoch 3 Batch 4500 Loss 1.297 Accuracy 0.4225\n",
            "Epoch 3 Batch 4550 Loss 1.299 Accuracy 0.4223\n",
            "Epoch 3 Batch 4600 Loss 1.3 Accuracy 0.4221\n",
            "Epoch 3 Batch 4650 Loss 1.302 Accuracy 0.4219\n",
            "Epoch 3 Batch 4700 Loss 1.303 Accuracy 0.4217\n",
            "Epoch 3 Batch 4750 Loss 1.304 Accuracy 0.4215\n",
            "Epoch 3 Batch 4800 Loss 1.306 Accuracy 0.4213\n",
            "Epoch 3 Batch 4850 Loss 1.307 Accuracy 0.4211\n",
            "Epoch 3 Batch 4900 Loss 1.309 Accuracy 0.4208\n",
            "Epoch 3 Batch 4950 Loss 1.31 Accuracy 0.4206\n",
            "Epoch 3 Batch 5000 Loss 1.311 Accuracy 0.4204\n",
            "Epoch 3 Batch 5050 Loss 1.312 Accuracy 0.4201\n",
            "Epoch 3 Batch 5100 Loss 1.313 Accuracy 0.4199\n",
            "Epoch 3 Batch 5150 Loss 1.314 Accuracy 0.4196\n",
            "Epoch 3 Batch 5200 Loss 1.315 Accuracy 0.4193\n",
            "Epoch 3 Batch 5250 Loss 1.316 Accuracy 0.4191\n",
            "Epoch 3 Batch 5300 Loss 1.317 Accuracy 0.4188\n",
            "Epoch 3 Batch 5350 Loss 1.318 Accuracy 0.4186\n",
            "Epoch 3 Batch 5400 Loss 1.319 Accuracy 0.4183\n",
            "Epoch 3 Batch 5450 Loss 1.32 Accuracy 0.4181\n",
            "Epoch 3 Batch 5500 Loss 1.321 Accuracy 0.4179\n",
            "Epoch 3 Batch 5550 Loss 1.322 Accuracy 0.4177\n",
            "Saving checkpoints for epoch 3 at ./Checkpoints/ckpt-3\n",
            "Time taken for 1 epoch: 1467.4381580352783 secs\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.447 Accuracy 0.4112\n",
            "Epoch 4 Batch 50 Loss 1.45 Accuracy 0.3965\n",
            "Epoch 4 Batch 100 Loss 1.429 Accuracy 0.3981\n",
            "Epoch 4 Batch 150 Loss 1.422 Accuracy 0.4001\n",
            "Epoch 4 Batch 200 Loss 1.418 Accuracy 0.4008\n",
            "Epoch 4 Batch 250 Loss 1.415 Accuracy 0.4008\n",
            "Epoch 4 Batch 300 Loss 1.412 Accuracy 0.4013\n",
            "Epoch 4 Batch 350 Loss 1.414 Accuracy 0.4016\n",
            "Epoch 4 Batch 400 Loss 1.413 Accuracy 0.4021\n",
            "Epoch 4 Batch 450 Loss 1.412 Accuracy 0.4015\n",
            "Epoch 4 Batch 500 Loss 1.408 Accuracy 0.4019\n",
            "Epoch 4 Batch 550 Loss 1.408 Accuracy 0.4019\n",
            "Epoch 4 Batch 600 Loss 1.405 Accuracy 0.4025\n",
            "Epoch 4 Batch 650 Loss 1.401 Accuracy 0.4034\n",
            "Epoch 4 Batch 700 Loss 1.397 Accuracy 0.4043\n",
            "Epoch 4 Batch 750 Loss 1.392 Accuracy 0.4050\n",
            "Epoch 4 Batch 800 Loss 1.388 Accuracy 0.4060\n",
            "Epoch 4 Batch 850 Loss 1.381 Accuracy 0.4070\n",
            "Epoch 4 Batch 900 Loss 1.377 Accuracy 0.4079\n",
            "Epoch 4 Batch 950 Loss 1.372 Accuracy 0.4086\n",
            "Epoch 4 Batch 1000 Loss 1.368 Accuracy 0.4094\n",
            "Epoch 4 Batch 1050 Loss 1.365 Accuracy 0.4099\n",
            "Epoch 4 Batch 1100 Loss 1.361 Accuracy 0.4105\n",
            "Epoch 4 Batch 1150 Loss 1.358 Accuracy 0.4110\n",
            "Epoch 4 Batch 1200 Loss 1.354 Accuracy 0.4114\n",
            "Epoch 4 Batch 1250 Loss 1.35 Accuracy 0.4121\n",
            "Epoch 4 Batch 1300 Loss 1.346 Accuracy 0.4131\n",
            "Epoch 4 Batch 1350 Loss 1.342 Accuracy 0.4141\n",
            "Epoch 4 Batch 1400 Loss 1.338 Accuracy 0.4151\n",
            "Epoch 4 Batch 1450 Loss 1.334 Accuracy 0.4160\n",
            "Epoch 4 Batch 1500 Loss 1.331 Accuracy 0.4170\n",
            "Epoch 4 Batch 1550 Loss 1.329 Accuracy 0.4177\n",
            "Epoch 4 Batch 1600 Loss 1.325 Accuracy 0.4185\n",
            "Epoch 4 Batch 1650 Loss 1.323 Accuracy 0.4195\n",
            "Epoch 4 Batch 1700 Loss 1.32 Accuracy 0.4204\n",
            "Epoch 4 Batch 1750 Loss 1.316 Accuracy 0.4211\n",
            "Epoch 4 Batch 1800 Loss 1.314 Accuracy 0.4218\n",
            "Epoch 4 Batch 1850 Loss 1.311 Accuracy 0.4226\n",
            "Epoch 4 Batch 1900 Loss 1.307 Accuracy 0.4233\n",
            "Epoch 4 Batch 1950 Loss 1.304 Accuracy 0.4237\n",
            "Epoch 4 Batch 2000 Loss 1.3 Accuracy 0.4244\n",
            "Epoch 4 Batch 2050 Loss 1.297 Accuracy 0.4248\n",
            "Epoch 4 Batch 2100 Loss 1.292 Accuracy 0.4253\n",
            "Epoch 4 Batch 2150 Loss 1.288 Accuracy 0.4257\n",
            "Epoch 4 Batch 2200 Loss 1.283 Accuracy 0.4261\n",
            "Epoch 4 Batch 2250 Loss 1.279 Accuracy 0.4266\n",
            "Epoch 4 Batch 2300 Loss 1.275 Accuracy 0.4270\n",
            "Epoch 4 Batch 2350 Loss 1.271 Accuracy 0.4275\n",
            "Epoch 4 Batch 2400 Loss 1.267 Accuracy 0.4279\n",
            "Epoch 4 Batch 2450 Loss 1.264 Accuracy 0.4283\n",
            "Epoch 4 Batch 2500 Loss 1.26 Accuracy 0.4288\n",
            "Epoch 4 Batch 2550 Loss 1.256 Accuracy 0.4294\n",
            "Epoch 4 Batch 2600 Loss 1.252 Accuracy 0.4300\n",
            "Epoch 4 Batch 2650 Loss 1.248 Accuracy 0.4306\n",
            "Epoch 4 Batch 2700 Loss 1.244 Accuracy 0.4311\n",
            "Epoch 4 Batch 2750 Loss 1.24 Accuracy 0.4318\n",
            "Epoch 4 Batch 2800 Loss 1.236 Accuracy 0.4324\n",
            "Epoch 4 Batch 2850 Loss 1.233 Accuracy 0.4327\n",
            "Epoch 4 Batch 2900 Loss 1.23 Accuracy 0.4331\n",
            "Epoch 4 Batch 2950 Loss 1.228 Accuracy 0.4335\n",
            "Epoch 4 Batch 3000 Loss 1.225 Accuracy 0.4337\n",
            "Epoch 4 Batch 3050 Loss 1.223 Accuracy 0.4340\n",
            "Epoch 4 Batch 3100 Loss 1.221 Accuracy 0.4342\n",
            "Epoch 4 Batch 3150 Loss 1.219 Accuracy 0.4344\n",
            "Epoch 4 Batch 3200 Loss 1.217 Accuracy 0.4346\n",
            "Epoch 4 Batch 3250 Loss 1.216 Accuracy 0.4347\n",
            "Epoch 4 Batch 3300 Loss 1.215 Accuracy 0.4348\n",
            "Epoch 4 Batch 3350 Loss 1.213 Accuracy 0.4351\n",
            "Epoch 4 Batch 3400 Loss 1.212 Accuracy 0.4352\n",
            "Epoch 4 Batch 3450 Loss 1.211 Accuracy 0.4353\n",
            "Epoch 4 Batch 3500 Loss 1.209 Accuracy 0.4355\n",
            "Epoch 4 Batch 3550 Loss 1.208 Accuracy 0.4357\n",
            "Epoch 4 Batch 3600 Loss 1.207 Accuracy 0.4359\n",
            "Epoch 4 Batch 3650 Loss 1.206 Accuracy 0.4360\n",
            "Epoch 4 Batch 3700 Loss 1.205 Accuracy 0.4361\n",
            "Epoch 4 Batch 3750 Loss 1.204 Accuracy 0.4363\n",
            "Epoch 4 Batch 3800 Loss 1.203 Accuracy 0.4364\n",
            "Epoch 4 Batch 3850 Loss 1.202 Accuracy 0.4365\n",
            "Epoch 4 Batch 3900 Loss 1.201 Accuracy 0.4367\n",
            "Epoch 4 Batch 3950 Loss 1.2 Accuracy 0.4369\n",
            "Epoch 4 Batch 4000 Loss 1.199 Accuracy 0.4370\n",
            "Epoch 4 Batch 4050 Loss 1.199 Accuracy 0.4370\n",
            "Epoch 4 Batch 4100 Loss 1.199 Accuracy 0.4369\n",
            "Epoch 4 Batch 4150 Loss 1.2 Accuracy 0.4368\n",
            "Epoch 4 Batch 4200 Loss 1.201 Accuracy 0.4367\n",
            "Epoch 4 Batch 4250 Loss 1.202 Accuracy 0.4365\n",
            "Epoch 4 Batch 4300 Loss 1.203 Accuracy 0.4363\n",
            "Epoch 4 Batch 4350 Loss 1.204 Accuracy 0.4361\n",
            "Epoch 4 Batch 4400 Loss 1.206 Accuracy 0.4359\n",
            "Epoch 4 Batch 4450 Loss 1.208 Accuracy 0.4356\n",
            "Epoch 4 Batch 4500 Loss 1.209 Accuracy 0.4353\n",
            "Epoch 4 Batch 4550 Loss 1.211 Accuracy 0.4350\n",
            "Epoch 4 Batch 4600 Loss 1.212 Accuracy 0.4348\n",
            "Epoch 4 Batch 4650 Loss 1.214 Accuracy 0.4346\n",
            "Epoch 4 Batch 4700 Loss 1.216 Accuracy 0.4343\n",
            "Epoch 4 Batch 4750 Loss 1.217 Accuracy 0.4340\n",
            "Epoch 4 Batch 4800 Loss 1.219 Accuracy 0.4337\n",
            "Epoch 4 Batch 4850 Loss 1.22 Accuracy 0.4335\n",
            "Epoch 4 Batch 4900 Loss 1.222 Accuracy 0.4332\n",
            "Epoch 4 Batch 4950 Loss 1.223 Accuracy 0.4330\n",
            "Epoch 4 Batch 5000 Loss 1.224 Accuracy 0.4327\n",
            "Epoch 4 Batch 5050 Loss 1.226 Accuracy 0.4325\n",
            "Epoch 4 Batch 5100 Loss 1.227 Accuracy 0.4322\n",
            "Epoch 4 Batch 5150 Loss 1.229 Accuracy 0.4320\n",
            "Epoch 4 Batch 5200 Loss 1.23 Accuracy 0.4317\n",
            "Epoch 4 Batch 5250 Loss 1.231 Accuracy 0.4314\n",
            "Epoch 4 Batch 5300 Loss 1.233 Accuracy 0.4311\n",
            "Epoch 4 Batch 5350 Loss 1.234 Accuracy 0.4308\n",
            "Epoch 4 Batch 5400 Loss 1.235 Accuracy 0.4306\n",
            "Epoch 4 Batch 5450 Loss 1.236 Accuracy 0.4303\n",
            "Epoch 4 Batch 5500 Loss 1.237 Accuracy 0.4301\n",
            "Epoch 4 Batch 5550 Loss 1.239 Accuracy 0.4298\n",
            "Saving checkpoints for epoch 4 at ./Checkpoints/ckpt-4\n",
            "Time taken for 1 epoch: 1472.9382519721985 secs\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.508 Accuracy 0.4161\n",
            "Epoch 5 Batch 50 Loss 1.387 Accuracy 0.4081\n",
            "Epoch 5 Batch 100 Loss 1.372 Accuracy 0.4111\n",
            "Epoch 5 Batch 150 Loss 1.361 Accuracy 0.4113\n",
            "Epoch 5 Batch 200 Loss 1.358 Accuracy 0.4117\n",
            "Epoch 5 Batch 250 Loss 1.358 Accuracy 0.4107\n",
            "Epoch 5 Batch 300 Loss 1.358 Accuracy 0.4113\n",
            "Epoch 5 Batch 350 Loss 1.354 Accuracy 0.4114\n",
            "Epoch 5 Batch 400 Loss 1.35 Accuracy 0.4117\n",
            "Epoch 5 Batch 450 Loss 1.347 Accuracy 0.4120\n",
            "Epoch 5 Batch 500 Loss 1.344 Accuracy 0.4119\n",
            "Epoch 5 Batch 550 Loss 1.344 Accuracy 0.4116\n",
            "Epoch 5 Batch 600 Loss 1.341 Accuracy 0.4120\n",
            "Epoch 5 Batch 650 Loss 1.336 Accuracy 0.4123\n",
            "Epoch 5 Batch 700 Loss 1.331 Accuracy 0.4127\n",
            "Epoch 5 Batch 750 Loss 1.327 Accuracy 0.4134\n",
            "Epoch 5 Batch 800 Loss 1.325 Accuracy 0.4141\n",
            "Epoch 5 Batch 850 Loss 1.32 Accuracy 0.4151\n",
            "Epoch 5 Batch 900 Loss 1.316 Accuracy 0.4161\n",
            "Epoch 5 Batch 950 Loss 1.312 Accuracy 0.4170\n",
            "Epoch 5 Batch 1000 Loss 1.309 Accuracy 0.4175\n",
            "Epoch 5 Batch 1050 Loss 1.305 Accuracy 0.4183\n",
            "Epoch 5 Batch 1100 Loss 1.301 Accuracy 0.4189\n",
            "Epoch 5 Batch 1150 Loss 1.298 Accuracy 0.4195\n",
            "Epoch 5 Batch 1200 Loss 1.296 Accuracy 0.4202\n",
            "Epoch 5 Batch 1250 Loss 1.291 Accuracy 0.4208\n",
            "Epoch 5 Batch 1300 Loss 1.288 Accuracy 0.4218\n",
            "Epoch 5 Batch 1350 Loss 1.285 Accuracy 0.4229\n",
            "Epoch 5 Batch 1400 Loss 1.281 Accuracy 0.4238\n",
            "Epoch 5 Batch 1450 Loss 1.277 Accuracy 0.4247\n",
            "Epoch 5 Batch 1500 Loss 1.274 Accuracy 0.4255\n",
            "Epoch 5 Batch 1550 Loss 1.27 Accuracy 0.4262\n",
            "Epoch 5 Batch 1600 Loss 1.268 Accuracy 0.4272\n",
            "Epoch 5 Batch 1650 Loss 1.265 Accuracy 0.4279\n",
            "Epoch 5 Batch 1700 Loss 1.262 Accuracy 0.4288\n",
            "Epoch 5 Batch 1750 Loss 1.259 Accuracy 0.4296\n",
            "Epoch 5 Batch 1800 Loss 1.257 Accuracy 0.4305\n",
            "Epoch 5 Batch 1850 Loss 1.254 Accuracy 0.4313\n",
            "Epoch 5 Batch 1900 Loss 1.25 Accuracy 0.4320\n",
            "Epoch 5 Batch 1950 Loss 1.247 Accuracy 0.4326\n",
            "Epoch 5 Batch 2000 Loss 1.243 Accuracy 0.4331\n",
            "Epoch 5 Batch 2050 Loss 1.239 Accuracy 0.4335\n",
            "Epoch 5 Batch 2100 Loss 1.235 Accuracy 0.4340\n",
            "Epoch 5 Batch 2150 Loss 1.231 Accuracy 0.4345\n",
            "Epoch 5 Batch 2200 Loss 1.226 Accuracy 0.4349\n",
            "Epoch 5 Batch 2250 Loss 1.222 Accuracy 0.4351\n",
            "Epoch 5 Batch 2300 Loss 1.218 Accuracy 0.4356\n",
            "Epoch 5 Batch 2350 Loss 1.214 Accuracy 0.4361\n",
            "Epoch 5 Batch 2400 Loss 1.211 Accuracy 0.4366\n",
            "Epoch 5 Batch 2450 Loss 1.207 Accuracy 0.4370\n",
            "Epoch 5 Batch 2500 Loss 1.203 Accuracy 0.4375\n",
            "Epoch 5 Batch 2550 Loss 1.2 Accuracy 0.4381\n",
            "Epoch 5 Batch 2600 Loss 1.196 Accuracy 0.4387\n",
            "Epoch 5 Batch 2650 Loss 1.193 Accuracy 0.4392\n",
            "Epoch 5 Batch 2700 Loss 1.189 Accuracy 0.4398\n",
            "Epoch 5 Batch 2750 Loss 1.185 Accuracy 0.4403\n",
            "Epoch 5 Batch 2800 Loss 1.181 Accuracy 0.4408\n",
            "Epoch 5 Batch 2850 Loss 1.178 Accuracy 0.4412\n",
            "Epoch 5 Batch 2900 Loss 1.175 Accuracy 0.4416\n",
            "Epoch 5 Batch 2950 Loss 1.173 Accuracy 0.4419\n",
            "Epoch 5 Batch 3000 Loss 1.17 Accuracy 0.4422\n",
            "Epoch 5 Batch 3050 Loss 1.168 Accuracy 0.4425\n",
            "Epoch 5 Batch 3100 Loss 1.166 Accuracy 0.4426\n",
            "Epoch 5 Batch 3150 Loss 1.165 Accuracy 0.4428\n",
            "Epoch 5 Batch 3200 Loss 1.163 Accuracy 0.4429\n",
            "Epoch 5 Batch 3250 Loss 1.161 Accuracy 0.4430\n",
            "Epoch 5 Batch 3300 Loss 1.16 Accuracy 0.4432\n",
            "Epoch 5 Batch 3350 Loss 1.158 Accuracy 0.4433\n",
            "Epoch 5 Batch 3400 Loss 1.157 Accuracy 0.4434\n",
            "Epoch 5 Batch 3450 Loss 1.156 Accuracy 0.4435\n",
            "Epoch 5 Batch 3500 Loss 1.155 Accuracy 0.4437\n",
            "Epoch 5 Batch 3550 Loss 1.153 Accuracy 0.4438\n",
            "Epoch 5 Batch 3600 Loss 1.153 Accuracy 0.4439\n",
            "Epoch 5 Batch 3650 Loss 1.152 Accuracy 0.4441\n",
            "Epoch 5 Batch 3700 Loss 1.151 Accuracy 0.4442\n",
            "Epoch 5 Batch 3750 Loss 1.149 Accuracy 0.4444\n",
            "Epoch 5 Batch 3800 Loss 1.148 Accuracy 0.4445\n",
            "Epoch 5 Batch 3850 Loss 1.148 Accuracy 0.4446\n",
            "Epoch 5 Batch 3900 Loss 1.147 Accuracy 0.4448\n",
            "Epoch 5 Batch 3950 Loss 1.146 Accuracy 0.4448\n",
            "Epoch 5 Batch 4000 Loss 1.145 Accuracy 0.4450\n",
            "Epoch 5 Batch 4050 Loss 1.145 Accuracy 0.4450\n",
            "Epoch 5 Batch 4100 Loss 1.146 Accuracy 0.4449\n",
            "Epoch 5 Batch 4150 Loss 1.147 Accuracy 0.4447\n",
            "Epoch 5 Batch 4200 Loss 1.148 Accuracy 0.4445\n",
            "Epoch 5 Batch 4250 Loss 1.149 Accuracy 0.4443\n",
            "Epoch 5 Batch 4300 Loss 1.151 Accuracy 0.4441\n",
            "Epoch 5 Batch 4350 Loss 1.152 Accuracy 0.4438\n",
            "Epoch 5 Batch 4400 Loss 1.154 Accuracy 0.4436\n",
            "Epoch 5 Batch 4450 Loss 1.155 Accuracy 0.4433\n",
            "Epoch 5 Batch 4500 Loss 1.157 Accuracy 0.4431\n",
            "Epoch 5 Batch 4550 Loss 1.159 Accuracy 0.4428\n",
            "Epoch 5 Batch 4600 Loss 1.161 Accuracy 0.4426\n",
            "Epoch 5 Batch 4650 Loss 1.163 Accuracy 0.4423\n",
            "Epoch 5 Batch 4700 Loss 1.165 Accuracy 0.4420\n",
            "Epoch 5 Batch 4750 Loss 1.167 Accuracy 0.4418\n",
            "Epoch 5 Batch 4800 Loss 1.168 Accuracy 0.4415\n",
            "Epoch 5 Batch 4850 Loss 1.17 Accuracy 0.4413\n",
            "Epoch 5 Batch 4900 Loss 1.171 Accuracy 0.4410\n",
            "Epoch 5 Batch 4950 Loss 1.173 Accuracy 0.4407\n",
            "Epoch 5 Batch 5000 Loss 1.174 Accuracy 0.4405\n",
            "Epoch 5 Batch 5050 Loss 1.176 Accuracy 0.4402\n",
            "Epoch 5 Batch 5100 Loss 1.177 Accuracy 0.4400\n",
            "Epoch 5 Batch 5150 Loss 1.179 Accuracy 0.4397\n",
            "Epoch 5 Batch 5200 Loss 1.18 Accuracy 0.4394\n",
            "Epoch 5 Batch 5250 Loss 1.182 Accuracy 0.4391\n",
            "Epoch 5 Batch 5300 Loss 1.183 Accuracy 0.4387\n",
            "Epoch 5 Batch 5300 Loss 1.183 Accuracy 0.4387\n",
            "Epoch 5 Batch 5350 Loss 1.184 Accuracy 0.4384\n",
            "Epoch 5 Batch 5350 Loss 1.184 Accuracy 0.4384\n",
            "Epoch 5 Batch 5400 Loss 1.185 Accuracy 0.4381\n",
            "Epoch 5 Batch 5400 Loss 1.185 Accuracy 0.4381\n",
            "Epoch 5 Batch 5450 Loss 1.187 Accuracy 0.4379\n",
            "Epoch 5 Batch 5450 Loss 1.187 Accuracy 0.4379\n",
            "Epoch 5 Batch 5500 Loss 1.188 Accuracy 0.4376\n",
            "Epoch 5 Batch 5500 Loss 1.188 Accuracy 0.4376\n",
            "Epoch 5 Batch 5550 Loss 1.189 Accuracy 0.4373\n",
            "Epoch 5 Batch 5550 Loss 1.189 Accuracy 0.4373\n",
            "Saving checkpoints for epoch 5 at ./Checkpoints/ckpt-5\n",
            "Time taken for 1 epoch: 1468.3548634052277 secs\n",
            "Saving checkpoints for epoch 5 at ./Checkpoints/ckpt-5\n",
            "Time taken for 1 epoch: 1468.3548634052277 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_swClvJWTw6"
      },
      "source": [
        "def evaluate(input_sentence):\n",
        "    input_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(input_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(input_sentence,axis=0)\n",
        "    output = tf.expand_dims([VOCAB_SIZE_IT-2],axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        my_predictions = transformer(enc_input,output,False) # (1,seq_length,vocab_size_it)\n",
        "        prediction = my_predictions[:,-1:,:]\n",
        "        prediction_id = tf.cast(tf.argmax(prediction,axis=-1) , tf.int32)\n",
        "        if prediction_id == VOCAB_SIZE_IT-1:\n",
        "            return tf.squeeze(output,axis=0)\n",
        "        output = tf.concat([output,prediction_id],axis=-1)\n",
        "    return tf.squeeze(output,axis=0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtJy9cxx-4Jm"
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    predicted_sentence = tokenizer_it.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_IT-2])\n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w75cn1QzHuuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02b94c8-627f-4059-fc65-c5a7aa172b70"
      },
      "source": [
        "translate(\"Working from home is really hard\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: Working from home is really hard\n",
            "Predicted translation: Lavori a casa si trova davvero molto difficile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pm5XKPjOXk4"
      },
      "source": [
        "# function ConnectButton(){\n",
        "#     console.log(\"Connect pushed\"); \n",
        "#     document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "# }\n",
        "# undefined\n",
        "# setInterval(ConnectButton,60000);"
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}